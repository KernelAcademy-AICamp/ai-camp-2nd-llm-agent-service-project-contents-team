"""
Analysis Agents

Text, Visual, Engagement ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” Agentë“¤
"""

import logging
from typing import List, Dict, Any
from .schemas import UnifiedContent
from ..utils.vertex_ai_client import get_vertex_client

logger = logging.getLogger(__name__)


# ===== Layer 3: Analysis Agents =====

class TextAnalyzerAgent:
    """í…ìŠ¤íŠ¸ ë¶„ì„ Agent"""

    def __init__(self):
        self.vertex_client = get_vertex_client()

    async def analyze(self, contents: List[UnifiedContent]) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¶„ì„

        Args:
            contents: í†µí•©ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸

        Returns:
            í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
            {
                "writing_style": str,
                "tone": str,
                "sentence_patterns": List[str],
                "formality_score": int,
                "warmth_score": int,
                "enthusiasm_score": int,
                "signature_phrases": List[str],
                "emoji_usage": Dict[str, Any],
                "keyword_frequency": Dict[str, int]
            }
        """
        try:
            if not contents:
                logger.warning("âš ï¸ [Text Analyzer] ë¶„ì„í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
                return self._get_default_analysis()

            logger.info(f"ğŸ“ [Text Analyzer] {len(contents)}ê°œ ì½˜í…ì¸  í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ê²°í•©
            texts = []
            for content in contents:
                text_parts = []
                if content.title:
                    text_parts.append(f"ì œëª©: {content.title}")
                if content.body_text:
                    text_parts.append(f"ë³¸ë¬¸: {content.body_text[:1000]}")  # ê° ë³¸ë¬¸ì€ ìµœëŒ€ 1000ì

                if text_parts:
                    texts.append("\n".join(text_parts))

            if not texts:
                logger.warning("âš ï¸ [Text Analyzer] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return self._get_default_analysis()

            # í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ê²°í•© (ìµœëŒ€ 20000ì)
            combined_text = "\n\n---\n\n".join(texts)[:20000]

            # Geminië¡œ í…ìŠ¤íŠ¸ ë¶„ì„
            prompt = f"""ë‹¹ì‹ ì€ ë¸Œëœë“œ í†¤ì•¤ë§¤ë„ˆ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë¸Œëœë“œì˜ ì½˜í…ì¸  í…ìŠ¤íŠ¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ê³¼ í†¤ì„ íŒŒì•…í•´ì£¼ì„¸ìš”.

===== ì½˜í…ì¸  í…ìŠ¤íŠ¸ =====
{combined_text}
============================

ë‹¤ìŒ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{{
  "writing_style": "ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ (ì˜ˆ: ìŠ¤í† ë¦¬í…”ë§ ì¤‘ì‹¬, ì •ë³´ ì „ë‹¬í˜•, ëŒ€í™”í˜•)",
  "tone": "ì „ì²´ì ì¸ í†¤ (ì˜ˆ: ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ, ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆëŠ”)",
  "sentence_patterns": ["ë¬¸ì¥ íŒ¨í„´ 1 (ì˜ˆ: ~í•´ìš”ì²´)", "ë¬¸ì¥ íŒ¨í„´ 2"],
  "formality_score": ê²©ì‹ ìˆ˜ì¤€ (0-100, 0=ë§¤ìš° ìºì£¼ì–¼, 100=ë§¤ìš° ê²©ì‹),
  "warmth_score": ë”°ëœ»í•¨ ìˆ˜ì¤€ (0-100, 0=ì°¨ê°€ìš´, 100=ë§¤ìš° ë”°ëœ»í•œ),
  "enthusiasm_score": ì—´ì • ìˆ˜ì¤€ (0-100, 0=ì°¨ë¶„í•œ, 100=ì—´ì •ì ì¸),
  "signature_phrases": ["ì‹œê·¸ë‹ˆì²˜ í‘œí˜„ 1", "ì‹œê·¸ë‹ˆì²˜ í‘œí˜„ 2", "ì‹œê·¸ë‹ˆì²˜ í‘œí˜„ 3"],
  "emoji_usage": {{
    "frequency": "ì‚¬ìš© ë¹ˆë„ (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ/ì—†ìŒ)",
    "preferred_emojis": ["ìì£¼ ì“°ëŠ” ì´ëª¨ì§€ 1", "ì´ëª¨ì§€ 2"]
  }},
  "keyword_frequency": {{
    "í‚¤ì›Œë“œ1": ë¹ˆë„ìˆ˜,
    "í‚¤ì›Œë“œ2": ë¹ˆë„ìˆ˜
  }}
}}

**ì¤‘ìš”**: ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

            analysis_result = await self.vertex_client.generate_json(prompt, temperature=0.3)
            logger.info("âœ… [Text Analyzer] í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
            return analysis_result

        except Exception as e:
            logger.error(f"âŒ [Text Analyzer] ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_analysis()

    def _get_default_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "writing_style": "ì •ë³´ ì „ë‹¬í˜•",
            "tone": "ì¤‘ë¦½ì ",
            "sentence_patterns": ["í‘œì¤€ì–´ì²´"],
            "formality_score": 50,
            "warmth_score": 50,
            "enthusiasm_score": 50,
            "signature_phrases": [],
            "emoji_usage": {"frequency": "ì—†ìŒ", "preferred_emojis": []},
            "keyword_frequency": {}
        }


class VisualAnalyzerAgent:
    """ë¹„ì£¼ì–¼ ë¶„ì„ Agent"""

    def __init__(self):
        self.vertex_client = get_vertex_client()

    async def analyze(self, contents: List[UnifiedContent]) -> Dict[str, Any]:
        """
        ë¹„ì£¼ì–¼ ì½˜í…ì¸  ë¶„ì„

        Args:
            contents: í†µí•©ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¹„ì£¼ì–¼ ë¶„ì„ ê²°ê³¼
            {
                "has_visual_content": bool,
                "primary_visual_type": str,
                "color_palette": List[str],
                "image_style": str,
                "composition_style": str,
                "visual_themes": List[str]
            }
        """
        try:
            if not contents:
                logger.warning("âš ï¸ [Visual Analyzer] ë¶„ì„í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
                return self._get_default_analysis()

            logger.info(f"ğŸ¨ [Visual Analyzer] {len(contents)}ê°œ ì½˜í…ì¸  ë¹„ì£¼ì–¼ ë¶„ì„ ì‹œì‘")

            # ë¹„ì£¼ì–¼ ì½˜í…ì¸  ì¶”ì¶œ
            visual_contents = [c for c in contents if c.media is not None]

            if not visual_contents:
                logger.info("â„¹ï¸ [Visual Analyzer] ë¹„ì£¼ì–¼ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ë¡ ")
                return await self._analyze_from_text(contents)

            # ë¹„ì£¼ì–¼ ì½˜í…ì¸  ìš”ì•½
            visual_summary = []
            for content in visual_contents:
                summary = f"""
í”Œë«í¼: {content.platform}
ë¯¸ë””ì–´ íƒ€ì…: {content.media.type}
ë¯¸ë””ì–´ ê°œìˆ˜: {content.media.count}
ìº¡ì…˜/ì„¤ëª…: {content.body_text[:200] if content.body_text else 'N/A'}
"""
                visual_summary.append(summary)

            combined_summary = "\n---\n".join(visual_summary)

            # Geminië¡œ ë¹„ì£¼ì–¼ ë¶„ì„ (í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ë¡ )
            prompt = f"""ë‹¹ì‹ ì€ ë¸Œëœë“œ ë¹„ì£¼ì–¼ ìŠ¤íƒ€ì¼ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë¸Œëœë“œì˜ ë¹„ì£¼ì–¼ ì½˜í…ì¸  ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œê°ì  ìŠ¤íƒ€ì¼ì„ íŒŒì•…í•´ì£¼ì„¸ìš”.

===== ë¹„ì£¼ì–¼ ì½˜í…ì¸  ì •ë³´ =====
{combined_summary}
===============================

ìº¡ì…˜ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ë¡ í•´ì£¼ì„¸ìš”:

{{
  "has_visual_content": true,
  "primary_visual_type": "ì£¼ìš” ë¹„ì£¼ì–¼ íƒ€ì… (image/video)",
  "color_palette": ["ì¶”ë¡ ëœ HEX ìƒ‰ìƒ ì½”ë“œ"],
  "image_style": "ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ (ì˜ˆ: ë°ê³  í™”ì‚¬í•œ, ë¯¸ë‹ˆë©€, ë¹ˆí‹°ì§€)",
  "composition_style": "êµ¬ë„ ìŠ¤íƒ€ì¼ (ì˜ˆ: ì¤‘ì•™ ì •ë ¬, ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒ)",
  "visual_themes": ["ë¹„ì£¼ì–¼ í…Œë§ˆ 1", "ë¹„ì£¼ì–¼ í…Œë§ˆ 2"]
}}

**ì¤‘ìš”**: ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""

            analysis_result = await self.vertex_client.generate_json(prompt, temperature=0.3)
            logger.info("âœ… [Visual Analyzer] ë¹„ì£¼ì–¼ ë¶„ì„ ì™„ë£Œ")
            return analysis_result

        except Exception as e:
            logger.error(f"âŒ [Visual Analyzer] ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_analysis()

    async def _analyze_from_text(self, contents: List[UnifiedContent]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¹„ì£¼ì–¼ ìŠ¤íƒ€ì¼ ì¶”ë¡ """
        # ë¹„ì£¼ì–¼ ì½˜í…ì¸ ê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ë¡ 
        texts = [c.body_text[:500] for c in contents if c.body_text]
        combined_text = "\n\n".join(texts)[:5000]

        prompt = f"""ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì´ ë¸Œëœë“œê°€ ì„ í˜¸í•  ê²ƒ ê°™ì€ ë¹„ì£¼ì–¼ ìŠ¤íƒ€ì¼ì„ ì¶”ë¡ í•´ì£¼ì„¸ìš”.

{combined_text}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "has_visual_content": false,
  "primary_visual_type": "none",
  "color_palette": ["ì¶”ì²œ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ HEX"],
  "image_style": "ì¶”ë¡ ëœ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼",
  "composition_style": "ì¶”ì²œ êµ¬ë„",
  "visual_themes": ["í…Œë§ˆ1", "í…Œë§ˆ2"]
}}
"""

        try:
            return await self.vertex_client.generate_json(prompt, temperature=0.5)
        except:
            return self._get_default_analysis()

    def _get_default_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "has_visual_content": False,
            "primary_visual_type": "none",
            "color_palette": ["#FFFFFF", "#000000"],
            "image_style": "ê¸°ë³¸ ìŠ¤íƒ€ì¼",
            "composition_style": "í‘œì¤€ ë ˆì´ì•„ì›ƒ",
            "visual_themes": []
        }


class EngagementAnalyzerAgent:
    """ì°¸ì—¬ ì§€í‘œ ë¶„ì„ Agent"""

    def __init__(self):
        self.vertex_client = get_vertex_client()

    async def analyze(self, contents: List[UnifiedContent]) -> Dict[str, Any]:
        """
        ì°¸ì—¬ ì§€í‘œ ë¶„ì„

        Args:
            contents: í†µí•©ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸

        Returns:
            ì°¸ì—¬ ë¶„ì„ ê²°ê³¼
            {
                "has_engagement_data": bool,
                "avg_engagement_rate": float,
                "top_performing_content_types": List[str],
                "engagement_insights": str
            }
        """
        try:
            if not contents:
                logger.warning("âš ï¸ [Engagement Analyzer] ë¶„ì„í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
                return self._get_default_analysis()

            logger.info(f"ğŸ“Š [Engagement Analyzer] {len(contents)}ê°œ ì½˜í…ì¸  ì°¸ì—¬ ì§€í‘œ ë¶„ì„ ì‹œì‘")

            # ì°¸ì—¬ ì§€í‘œê°€ ìˆëŠ” ì½˜í…ì¸  í•„í„°
            contents_with_engagement = [c for c in contents if c.engagement is not None]

            if not contents_with_engagement:
                logger.info("â„¹ï¸ [Engagement Analyzer] ì°¸ì—¬ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "has_engagement_data": False,
                    "avg_engagement_rate": 0.0,
                    "top_performing_content_types": [],
                    "engagement_insights": "ì°¸ì—¬ ì§€í‘œ ë°ì´í„° ì—†ìŒ"
                }

            # í†µê³„ ê³„ì‚°
            total_likes = sum(c.engagement.likes for c in contents_with_engagement)
            total_comments = sum(c.engagement.comments for c in contents_with_engagement)
            total_views = sum(c.engagement.views for c in contents_with_engagement)

            avg_likes = total_likes / len(contents_with_engagement)
            avg_comments = total_comments / len(contents_with_engagement)

            # í”Œë«í¼ë³„ ì„±ê³¼
            platform_performance = {}
            for content in contents_with_engagement:
                platform = content.platform
                if platform not in platform_performance:
                    platform_performance[platform] = []
                platform_performance[platform].append({
                    'likes': content.engagement.likes,
                    'comments': content.engagement.comments
                })

            # ì„±ê³¼ ìš”ì•½
            summary = f"""
ì´ ì½˜í…ì¸  ìˆ˜: {len(contents_with_engagement)}
í‰ê·  ì¢‹ì•„ìš”: {avg_likes:.1f}
í‰ê·  ëŒ“ê¸€: {avg_comments:.1f}
ì´ ì¡°íšŒìˆ˜: {total_views}

í”Œë«í¼ë³„ ì„±ê³¼:
{platform_performance}
"""

            logger.info("âœ… [Engagement Analyzer] ì°¸ì—¬ ì§€í‘œ ë¶„ì„ ì™„ë£Œ")

            return {
                "has_engagement_data": True,
                "avg_engagement_rate": avg_likes + avg_comments,
                "top_performing_content_types": list(platform_performance.keys()),
                "engagement_insights": summary
            }

        except Exception as e:
            logger.error(f"âŒ [Engagement Analyzer] ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_analysis()

    def _get_default_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "has_engagement_data": False,
            "avg_engagement_rate": 0.0,
            "top_performing_content_types": [],
            "engagement_insights": "ë°ì´í„° ì—†ìŒ"
        }
