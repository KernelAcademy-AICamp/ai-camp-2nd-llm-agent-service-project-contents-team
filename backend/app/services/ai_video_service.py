"""
AI ë¹„ë””ì˜¤ ìƒì„± ì„œë¹„ìŠ¤
- Master Planning Agent: 4ë‹¨ê³„ Context Engineering íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±
  1. ProductAnalysisAgent: ì œí’ˆ ì´ë¯¸ì§€ ë¶„ì„
  2. StoryPlanningAgent: ìŠ¤í† ë¦¬ êµ¬ì¡° ì„ íƒ
  3. SceneDirectorAgent: ì¥ë©´ë³„ ì—°ì¶œ ì„¤ê³„
  4. QualityValidatorAgent: í’ˆì§ˆ ê²€ì¦ ë° ìë™ ìˆ˜ì •
- Image Generation: Gemini 2.5 Flash Image (ì¼ë°˜) / Gemini 3 Pro Image (í…ìŠ¤íŠ¸ íŠ¹í™”)
- Video Generation: Kling 2.1 Standard (via fal.ai) - Image-to-Video íŠ¸ëœì§€ì…˜ ìƒì„±
- Video Composition: moviepyë¡œ ìµœì¢… ë¹„ë””ì˜¤ í•©ì„± (ë¹ ë¥¸ ì»· ì „í™˜)
"""
import os
import json
import base64
import httpx
import asyncio
import random
from typing import List, Dict, Any, Optional
from pathlib import Path
import anthropic
import google.generativeai as genai
import vertexai
from vertexai.generative_models import GenerativeModel as VertexGenerativeModel, Part
from sqlalchemy.orm import Session
import fal_client

from ..models import VideoGenerationJob, User, BrandAnalysis
from ..logger import get_logger

logger = get_logger(__name__)

# Google Gemini ì„¤ì • (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Vertex AI ì§€ì—­ ì„¤ì • (ë©€í‹° ë¦¬ì „ ë¡œí…Œì´ì…˜ìœ¼ë¡œ ì¿¼í„° ë¶„ì‚°)
# ë¯¸êµ­ê³¼ ìœ ëŸ½ ì§€ì—­ë§Œ ì‚¬ìš©
AVAILABLE_REGIONS = [
    "europe-west4",      # ë„¤ëœë€ë“œ
    "us-west1",          # ì˜¤ë ˆê³¤
    "us-east4"           # ë²„ì§€ë‹ˆì•„
]

# ì„ íƒëœ ì§€ì—­ì„ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
SELECTED_LOCATION = None

# Vertex AI ì´ˆê¸°í™” (GOOGLE_APPLICATION_CREDENTIALS ì‚¬ìš©)
try:
    # í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆê³  ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì—­ì´ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ëœë¤ ì„ íƒ
    location = os.getenv("GOOGLE_CLOUD_LOCATION")
    if not location or location not in AVAILABLE_REGIONS:
        location = random.choice(AVAILABLE_REGIONS)
        logger.info(f"ğŸŒ Random region selected for quota distribution: {location}")

    SELECTED_LOCATION = location  # ì „ì—­ ë³€ìˆ˜ì— ì €ì¥

    vertexai.init(
        project=os.getenv("GOOGLE_CLOUD_PROJECT"),
        location=location
    )
    logger.info(f"Vertex AI initialized: project={os.getenv('GOOGLE_CLOUD_PROJECT')}, location={location}")
except Exception as e:
    logger.warning(f"Failed to initialize Vertex AI: {e}")


class MasterPlanningAgent:
    """
    Master Planning Agent
    - 4ë‹¨ê³„ Context Engineering íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±
    - 1ë‹¨ê³„: ì œí’ˆ ì´ë¯¸ì§€ ë¶„ì„ (ProductAnalysisAgent)
    - 2ë‹¨ê³„: ìŠ¤í† ë¦¬ êµ¬ì¡° ì„ íƒ (StoryPlanningAgent)
    - 3ë‹¨ê³„: ì¥ë©´ë³„ ì—°ì¶œ ì„¤ê³„ (SceneDirectorAgent)
    - 4ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ìë™ ìˆ˜ì • (QualityValidatorAgent)
    """

    def __init__(self, model: str = "gemini-2.5-flash"):
        self.model = model
        # ìƒˆë¡œìš´ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
        from ..video_agents import VideoStoryboardOrchestrator
        self.orchestrator = VideoStoryboardOrchestrator()

    async def analyze_and_plan(
        self,
        job: VideoGenerationJob,
        user: User,
        brand_analysis: Optional[BrandAnalysis],
        db: Session
    ) -> List[Dict[str, Any]]:
        """
        4ë‹¨ê³„ Context Engineering íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±

        Args:
            job: VideoGenerationJob ì¸ìŠ¤í„´ìŠ¤
            user: User ì¸ìŠ¤í„´ìŠ¤
            brand_analysis: BrandAnalysis ì¸ìŠ¤í„´ìŠ¤ (ìˆëŠ” ê²½ìš°)
            db: Database session

        Returns:
            List[Dict]: ìŠ¤í† ë¦¬ë³´ë“œ ì»· ë¦¬ìŠ¤íŠ¸
            [
                {
                    "cut": 1,
                    "scene_description": "...",
                    "image_prompt": "...",
                    "duration": 4.0
                },
                ...
            ]
        """
        try:
            logger.info(f"Starting Master Planning Agent (4-Stage Pipeline) for job {job.id}")

            # 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            storyboard_result = await self.orchestrator.generate_storyboard(
                job=job,
                user=user,
                brand_analysis=brand_analysis,
                db=db
            )

            # ìƒˆ êµ¬ì¡°: {shared_visual_context: {...}, storyboard: [...]}
            # DBì— ì „ì²´ êµ¬ì¡° ì €ì¥ (shared_visual_context í¬í•¨)
            job.storyboard = storyboard_result
            db.commit()

            # storyboard ë°°ì—´ ì¶”ì¶œ (í•˜ìœ„ ì²˜ë¦¬ìš©)
            storyboard = storyboard_result.get("storyboard", storyboard_result if isinstance(storyboard_result, list) else [])
            shared_context = storyboard_result.get("shared_visual_context", {})

            logger.info(f"Storyboard generated for job {job.id}: {len([s for s in storyboard if 'cut' in s])} cuts")
            if shared_context:
                logger.info(f"Shared visual context: {shared_context.get('primary_setting', 'N/A')}")

            return storyboard_result

        except Exception as e:
            logger.error(f"Error in Master Planning Agent for job {job.id}: {str(e)}")
            job.status = "failed"
            job.error_message = f"Planning failed: {str(e)}"
            db.commit()
            raise

    async def analyze_uploaded_image(self, image_data: Dict[str, str]) -> Dict[str, Any]:
        """
        ì—…ë¡œë“œëœ ì œí’ˆ ì´ë¯¸ì§€ì˜ ë¹„ì£¼ì–¼ íŠ¹ì§• ë¶„ì„

        Args:
            image_data: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°

        Returns:
            Dict: ì œí’ˆì˜ ë¹„ì£¼ì–¼ íŠ¹ì§•
            {
                "colors": ["white", "gold", "minimalist"],
                "style": "luxury premium aesthetic",
                "lighting": "soft natural lighting",
                "composition": "centered, minimalist",
                "key_elements": "golden cap, white bottle, marble background",
                "mood": "elegant, sophisticated"
            }
        """
        try:
            logger.info("Analyzing uploaded product image for visual features...")

            # Vertex AI Gemini ëª¨ë¸ ì´ˆê¸°í™”
            gemini_model = VertexGenerativeModel("gemini-2.5-flash")

            # image_dataë¥¼ PIL Imageë¡œ ë³€í™˜ í›„ Vertex AI Part ê°ì²´ë¡œ ë³€í™˜
            from PIL import Image
            import io

            image_bytes = base64.b64decode(image_data["data"])
            pil_image = Image.open(io.BytesIO(image_bytes))

            # PIL Image â†’ Vertex AI Part ê°ì²´ ë³€í™˜
            img_byte_arr = io.BytesIO()
            pil_image.save(img_byte_arr, format='JPEG')
            img_bytes = img_byte_arr.getvalue()

            image_part = Part.from_data(
                data=img_bytes,
                mime_type="image/jpeg"
            )

            # ì´ë¯¸ì§€ ë¶„ì„ í”„ë¡¬í”„íŠ¸
            analysis_prompt = """ì´ ì œí’ˆ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë¹„ì£¼ì–¼ íŠ¹ì§•ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{
  "colors": ["ì£¼ìš” ìƒ‰ìƒ 1", "ì£¼ìš” ìƒ‰ìƒ 2", "ì£¼ìš” ìƒ‰ìƒ 3"],
  "style": "ì „ì²´ì ì¸ ìŠ¤íƒ€ì¼ (ì˜ˆ: luxury premium, casual modern, minimalist, vintage ë“±)",
  "lighting": "ì¡°ëª… ìŠ¤íƒ€ì¼ (ì˜ˆ: soft natural lighting, dramatic studio lighting, bright daylight ë“±)",
  "composition": "êµ¬ë„ ë° ë ˆì´ì•„ì›ƒ (ì˜ˆ: centered, off-center, close-up, full view ë“±)",
  "key_elements": "ì£¼ìš” ì‹œê°ì  ìš”ì†Œë“¤ (ì˜ˆ: golden cap, white bottle, marble background)",
  "mood": "ì „ì²´ì ì¸ ë¶„ìœ„ê¸° (ì˜ˆ: elegant sophisticated, playful fun, professional clean ë“±)",
  "background": "ë°°ê²½ ìŠ¤íƒ€ì¼ (ì˜ˆ: marble texture, plain white, wooden surface ë“±)"
}

ì œí’ˆì˜ í•µì‹¬ ë¹„ì£¼ì–¼ ì •ì²´ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.
ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

            # Vertex AI Gemini API í˜¸ì¶œ
            response = gemini_model.generate_content([analysis_prompt, image_part])

            # ì‘ë‹µ íŒŒì‹±
            response_text = response.text
            logger.info(f"Image analysis response: {response_text[:200]}...")

            # JSON íŒŒì‹±
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            visual_features = json.loads(response_text)

            logger.info(f"âœ… Image analysis completed: {visual_features}")
            return visual_features

        except Exception as e:
            logger.error(f"Failed to analyze uploaded image: {str(e)}")
            # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "colors": ["natural"],
                "style": "professional",
                "lighting": "natural lighting",
                "composition": "centered",
                "key_elements": "product",
                "mood": "clean professional",
                "background": "neutral"
            }

    async def _download_and_encode_image(self, image_url: str) -> Dict[str, str]:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (HTTP/HTTPS) ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ì½ê¸° ë° base64 ì¸ì½”ë”©"""

        if image_url.startswith(("http://", "https://")):
            # HTTP/HTTPS URL - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
            async with httpx.AsyncClient() as client:
                response = await client.get(image_url)
                response.raise_for_status()

                content_type = response.headers.get("content-type", "image/jpeg")
                media_type = content_type.split("/")[-1]
                image_content = response.content
        else:
            # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ - íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì½ê¸°
            import mimetypes

            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
            # image_urlì´ "/uploads/..."ë¡œ ì‹œì‘í•˜ë¯€ë¡œ ì•ì˜ "/" ì œê±°
            file_path = Path(__file__).parent.parent.parent / image_url.lstrip("/")

            logger.info(f"Reading image from local filesystem: {file_path}")

            if not file_path.exists():
                raise FileNotFoundError(f"Image file not found: {file_path}")

            # íŒŒì¼ ì½ê¸°
            image_content = file_path.read_bytes()

            # MIME íƒ€ì… ì¶”ì¸¡
            mime_type, _ = mimetypes.guess_type(str(file_path))
            if mime_type and mime_type.startswith("image/"):
                media_type = mime_type.split("/")[-1]
            else:
                # í™•ì¥ìë¡œ ì¶”ì¸¡
                extension = file_path.suffix.lstrip(".")
                media_type = extension if extension else "jpeg"

            logger.info(f"Image loaded from filesystem: {len(image_content)} bytes, type: image/{media_type}")

        # base64 ì¸ì½”ë”©
        image_base64 = base64.b64encode(image_content).decode("utf-8")

        return {
            "type": "base64",
            "media_type": f"image/{media_type}",
            "data": image_base64
        }

    def _validate_visual_consistency(
        self,
        storyboard: List[Dict[str, Any]],
        visual_features: Dict[str, Any]
    ) -> None:
        """
        ìƒì„±ëœ ìŠ¤í† ë¦¬ë³´ë“œì˜ ë¹„ì£¼ì–¼ ì¼ê´€ì„± ê²€ì¦

        Args:
            storyboard: ìƒì„±ëœ ìŠ¤í† ë¦¬ë³´ë“œ
            visual_features: ì¶”ì¶œëœ ë¹„ì£¼ì–¼ íŠ¹ì§•

        Raises:
            ê²½ê³  ë¡œê·¸ë§Œ ì¶œë ¥ (ì‹¤íŒ¨í•˜ì§€ ì•ŠìŒ)
        """
        try:
            logger.info("Validating visual consistency of generated storyboard...")

            # ì»·ë§Œ í•„í„°ë§
            cuts = [item for item in storyboard if 'cut' in item]

            # AIê°€ ìƒì„±í•  ì»·ë“¤ë§Œ ê²€ì¦ (use_uploaded_imageê°€ Falseì¸ ê²ƒë“¤)
            ai_generated_cuts = [cut for cut in cuts if not cut.get('use_uploaded_image', False)]

            if not ai_generated_cuts:
                logger.info("No AI-generated cuts to validate (all cuts use uploaded image)")
                return

            # ë¹„ì£¼ì–¼ íŠ¹ì§•ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_to_check = []

            # ìƒ‰ìƒ
            if visual_features.get("colors"):
                colors = visual_features["colors"] if isinstance(visual_features["colors"], list) else [visual_features["colors"]]
                keywords_to_check.extend([c.lower() for c in colors])

            # ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œ
            if visual_features.get("style"):
                style_keywords = visual_features["style"].lower().split()
                keywords_to_check.extend(style_keywords)

            # ì¡°ëª… í‚¤ì›Œë“œ
            if visual_features.get("lighting"):
                lighting_keywords = visual_features["lighting"].lower().split()
                keywords_to_check.extend(lighting_keywords)

            logger.info(f"Checking for visual consistency keywords: {keywords_to_check[:10]}...")

            # ê° AI ìƒì„± ì»· ê²€ì¦
            inconsistent_cuts = []
            for cut in ai_generated_cuts:
                cut_number = cut.get('cut', 'unknown')
                image_prompt = cut.get('image_prompt', '').lower()

                # ì£¼ìš” í‚¤ì›Œë“œ ì¤‘ ì¼ë¶€ë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                found_keywords = [kw for kw in keywords_to_check if kw in image_prompt]

                if len(found_keywords) < max(1, len(keywords_to_check) // 3):
                    # ì£¼ìš” í‚¤ì›Œë“œì˜ 1/3 ë¯¸ë§Œë§Œ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê²½ê³ 
                    inconsistent_cuts.append({
                        'cut': cut_number,
                        'prompt': cut.get('image_prompt', '')[:100] + '...',
                        'found_keywords': found_keywords
                    })

            # ê²€ì¦ ê²°ê³¼ ë¡œê·¸
            if inconsistent_cuts:
                logger.warning(f"âš ï¸  {len(inconsistent_cuts)}/{len(ai_generated_cuts)} cuts may lack visual consistency:")
                for item in inconsistent_cuts[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    logger.warning(f"  - Cut {item['cut']}: found only {item['found_keywords']}")
                logger.warning(f"  Visual features may not be fully reflected in image prompts")
            else:
                logger.info(f"âœ… Visual consistency validated: {len(ai_generated_cuts)} AI-generated cuts checked")

        except Exception as e:
            logger.warning(f"Failed to validate visual consistency: {str(e)}")
            # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    def _prepare_brand_context(
        self,
        user: User,
        brand_analysis: Optional[BrandAnalysis],
        visual_features: Optional[Dict[str, Any]] = None
    ) -> str:
        """ë¸Œëœë“œ ë¶„ì„ ë°ì´í„° ë° ì œí’ˆ ë¹„ì£¼ì–¼ íŠ¹ì§•ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¤€ë¹„ (brand_profile_json ìš°ì„  ì‚¬ìš©)"""
        context_parts = []

        # brand_profile_json ìš°ì„  ì‚¬ìš©
        if brand_analysis and brand_analysis.brand_profile_json:
            profile = brand_analysis.brand_profile_json

            # Identity
            identity = profile.get('identity', {})
            if identity.get('brand_name'):
                context_parts.append(f"ë¸Œëœë“œëª…: {identity['brand_name']}")
            elif user.brand_name:
                context_parts.append(f"ë¸Œëœë“œëª…: {user.brand_name}")

            if identity.get('business_type'):
                context_parts.append(f"ì—…ì¢…: {identity['business_type']}")
            elif user.business_type:
                context_parts.append(f"ì—…ì¢…: {user.business_type}")

            if user.business_description:
                context_parts.append(f"ë¹„ì¦ˆë‹ˆìŠ¤ ì„¤ëª…: {user.business_description}")

            if identity.get('brand_personality'):
                context_parts.append(f"ë¸Œëœë“œ ì„±ê²©: {identity['brand_personality']}")
            if identity.get('target_audience'):
                context_parts.append(f"íƒ€ê²Ÿ ê³ ê°: {identity['target_audience']}")
            if identity.get('emotional_tone'):
                context_parts.append(f"ê°ì •ì  í†¤: {identity['emotional_tone']}")
            if identity.get('brand_values'):
                values = ", ".join(identity['brand_values']) if isinstance(identity['brand_values'], list) else identity['brand_values']
                context_parts.append(f"ë¸Œëœë“œ ê°€ì¹˜: {values}")

            # Tone of Voice (ìˆ˜ì¹˜í™”ëœ ì •ë³´ í™œìš©)
            tone = profile.get('tone_of_voice', {})
            if tone.get('formality') is not None:
                context_parts.append(f"ê²©ì‹ë„: {tone['formality']}/100 (0=ë§¤ìš° ìºì£¼ì–¼, 100=ë§¤ìš° ê²©ì‹ìˆëŠ”)")
            if tone.get('warmth') is not None:
                context_parts.append(f"ë”°ëœ»í•¨: {tone['warmth']}/100 (0=ì°¨ê°€ìš´, 100=ë§¤ìš° ë”°ëœ»í•œ)")
            if tone.get('enthusiasm') is not None:
                context_parts.append(f"ì—´ì •ë„: {tone['enthusiasm']}/100 (0=ì°¨ë¶„í•œ, 100=ì—´ì •ì ì¸)")
            if tone.get('sentence_style'):
                context_parts.append(f"ë¬¸ì¥ ìŠ¤íƒ€ì¼: {tone['sentence_style']}")
            if tone.get('signature_phrases'):
                phrases = ", ".join(tone['signature_phrases']) if isinstance(tone['signature_phrases'], list) else tone['signature_phrases']
                context_parts.append(f"ì‹œê·¸ë‹ˆì²˜ í‘œí˜„: {phrases}")

            # Content Strategy
            strategy = profile.get('content_strategy', {})
            if strategy.get('primary_topics'):
                topics = ", ".join(strategy['primary_topics']) if isinstance(strategy['primary_topics'], list) else strategy['primary_topics']
                context_parts.append(f"ì£¼ìš” ì£¼ì œ: {topics}")
            if strategy.get('content_structure'):
                context_parts.append(f"ì½˜í…ì¸  êµ¬ì¡°: {strategy['content_structure']}")
            if strategy.get('call_to_action_style'):
                context_parts.append(f"í–‰ë™ ìœ ë„ ë°©ì‹: {strategy['call_to_action_style']}")

            # Visual Style (ì˜ìƒ/ì´ë¯¸ì§€ ìƒì„±ì— ì¤‘ìš”!)
            visual = profile.get('visual_style', {})
            if visual.get('color_palette'):
                colors = ", ".join(visual['color_palette']) if isinstance(visual['color_palette'], list) else visual['color_palette']
                context_parts.append(f"ë¸Œëœë“œ ì»¬ëŸ¬ íŒ”ë ˆíŠ¸: {colors}")
            if visual.get('image_style'):
                context_parts.append(f"ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼: {visual['image_style']}")
            if visual.get('composition_style'):
                context_parts.append(f"êµ¬ë„ ìŠ¤íƒ€ì¼: {visual['composition_style']}")
            if visual.get('filter_preference'):
                context_parts.append(f"í•„í„° ì„ í˜¸ë„: {visual['filter_preference']}")

        else:
            # Fallback: User í…Œì´ë¸” ë° ê°œë³„ í•„ë“œ ì‚¬ìš©
            if user.brand_name:
                context_parts.append(f"ë¸Œëœë“œëª…: {user.brand_name}")
            if user.business_type:
                context_parts.append(f"ì—…ì¢…: {user.business_type}")
            if user.business_description:
                context_parts.append(f"ë¹„ì¦ˆë‹ˆìŠ¤ ì„¤ëª…: {user.business_description}")

            # ë¸Œëœë“œ ë¶„ì„ ì •ë³´ (ê°œë³„ í•„ë“œ)
            if brand_analysis:
                if brand_analysis.brand_tone:
                    context_parts.append(f"ë¸Œëœë“œ í†¤ì•¤ë§¤ë„ˆ: {brand_analysis.brand_tone}")
                if brand_analysis.target_audience:
                    context_parts.append(f"íƒ€ê²Ÿ ê³ ê°: {brand_analysis.target_audience}")
                if brand_analysis.emotional_tone:
                    context_parts.append(f"ê°ì •ì  í†¤: {brand_analysis.emotional_tone}")
                if brand_analysis.brand_values:
                    values = ", ".join(brand_analysis.brand_values) if isinstance(brand_analysis.brand_values, list) else brand_analysis.brand_values
                    context_parts.append(f"ë¸Œëœë“œ ê°€ì¹˜: {values}")

        # ì œí’ˆ ë¹„ì£¼ì–¼ íŠ¹ì§• (ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼)
        if visual_features:
            context_parts.append("\n[ì œí’ˆ ë¹„ì£¼ì–¼ íŠ¹ì§• - ëª¨ë“  ì»·ì—ì„œ ì¼ê´€ë˜ê²Œ ìœ ì§€í•´ì•¼ í•¨]")

            if visual_features.get("colors"):
                colors = ", ".join(visual_features["colors"]) if isinstance(visual_features["colors"], list) else visual_features["colors"]
                context_parts.append(f"ì£¼ìš” ìƒ‰ìƒ: {colors}")

            if visual_features.get("style"):
                context_parts.append(f"ë¹„ì£¼ì–¼ ìŠ¤íƒ€ì¼: {visual_features['style']}")

            if visual_features.get("lighting"):
                context_parts.append(f"ì¡°ëª… ìŠ¤íƒ€ì¼: {visual_features['lighting']}")

            if visual_features.get("composition"):
                context_parts.append(f"êµ¬ë„: {visual_features['composition']}")

            if visual_features.get("key_elements"):
                context_parts.append(f"í•µì‹¬ ì‹œê°ì  ìš”ì†Œ: {visual_features['key_elements']}")

            if visual_features.get("mood"):
                context_parts.append(f"ë¶„ìœ„ê¸°: {visual_features['mood']}")

            if visual_features.get("background"):
                context_parts.append(f"ë°°ê²½ ìŠ¤íƒ€ì¼: {visual_features['background']}")

        if not context_parts:
            return "ë¸Œëœë“œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì œí’ˆ ì´ë¯¸ì§€ì™€ ì„¤ëª…ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í† ë¦¬ë³´ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."

        return "\n".join(context_parts)

    async def _generate_storyboard(
        self,
        product_name: str,
        product_description: Optional[str],
        cut_count: int,
        duration_seconds: int,
        image_data: Dict[str, str],
        brand_context: str
    ) -> List[Dict[str, Any]]:
        """Claudeë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±"""

        # íŠ¸ëœì§€ì…˜ í‰ê·  ê¸¸ì´ ê³„ì‚° (ì»· ìˆ˜ - 1 = íŠ¸ëœì§€ì…˜ ìˆ˜)
        num_transitions = cut_count - 1
        avg_transition_duration = duration_seconds / num_transitions if num_transitions > 0 else 5.0
        cut_duration = 0.3  # ì»· ì´ë¯¸ì§€ëŠ” ì§§ê²Œ ê³ ì •

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ ì œí’ˆ ë§ˆì¼€íŒ… ë¹„ë””ì˜¤ì˜ ìŠ¤í† ë¦¬ë³´ë“œë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì œí’ˆ ì´ë¯¸ì§€ì™€ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬, {cut_count}ê°œì˜ ì»·ìœ¼ë¡œ êµ¬ì„±ëœ ì•½ {duration_seconds}ì´ˆ ê¸¸ì´ì˜ ë§ˆì¼€íŒ… ë¹„ë””ì˜¤ ìŠ¤í† ë¦¬ë³´ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

**â±ï¸ íƒ€ì´ë° êµ¬ì¡°:**
- ê° ì»· ì´ë¯¸ì§€: {cut_duration}ì´ˆ (ê³ ì •) - í‚¤ í”„ë ˆì„ì„ ì§§ê²Œ í‘œì‹œ
- íŠ¸ëœì§€ì…˜: í‰ê·  {avg_transition_duration:.1f}ì´ˆ - ì‹¤ì œ ì›€ì§ì„ê³¼ ì „í™˜ì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„
- ì´ ê¸¸ì´: ì•½ {duration_seconds}ì´ˆ

**ğŸ“– ìŠ¤í† ë¦¬í…”ë§ í”„ë ˆì„ì›Œí¬ (í•„ìˆ˜ ì„ íƒ):**

ì œí’ˆê³¼ ë¸Œëœë“œ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ **ê°€ì¥ ì í•©í•œ ìŠ¤í† ë¦¬ êµ¬ì¡° 1ê°€ì§€**ë¥¼ ì„ íƒí•˜ê³  ë”°ë¥´ì„¸ìš”:

**1. Problem-Solution (ë¬¸ì œ-í•´ê²°)**
- ì í•©í•œ ì œí’ˆ: ê¸°ëŠ¥ì„± ì œí’ˆ, ìƒí™œìš©í’ˆ, ê±´ê°•ì‹í’ˆ, ì—ë„ˆì§€ ë“œë§í¬
- êµ¬ì¡°: ë¬¸ì œ ìƒí™© ì œì‹œ â†’ ì œí’ˆ ë“±ì¥ â†’ í•´ê²° ê³¼ì • â†’ ê¸ì •ì  ê²°ê³¼
- ì˜ˆì‹œ: í”¼ê³¤í•œ ì•„ì¹¨ â†’ ì—ë„ˆì§€ ë“œë§í¬ â†’ í™œê¸°ì°¬ í•˜ë£¨

**2. Before-After (ë³€í™”)**
- ì í•©í•œ ì œí’ˆ: í™”ì¥í’ˆ, í”¼íŠ¸ë‹ˆìŠ¤, ì²­ì†Œìš©í’ˆ, ì—ë„ˆì§€ ë“œë§í¬
- êµ¬ì¡°: ì‚¬ìš© ì „ ìƒíƒœ â†’ ì œí’ˆ ì‚¬ìš© â†’ ë³€í™” ê³¼ì • â†’ ì‚¬ìš© í›„ ê²°ê³¼
- ì˜ˆì‹œ: ì§€ì¹œ í”¼ë¶€ â†’ ìŠ¤í‚¨ì¼€ì–´ â†’ ì´‰ì´‰í•œ í”¼ë¶€

**3. Process/Creation (ì œì‘ ê³¼ì •)**
- ì í•©í•œ ì œí’ˆ: ìˆ˜ì œ ìŒì‹, ì¹´í˜ ìŒë£Œ, ë””ì €íŠ¸, ìˆ˜ì œí’ˆ, ì•„í‹°ì¦Œ ì œí’ˆ
- êµ¬ì¡°: ì¬ë£Œ/ì¤€ë¹„ â†’ ì œì‘ ê³¼ì • (ì—­ë™ì  ìˆœê°„) â†’ ì™„ì„±í’ˆ
- íŠ¹ì§•: ASMR ìš”ì†Œ, ì‹œê°ì  ë§Œì¡±ê° (ì¸µ ë‚˜ë‰˜ê¸°, ìƒ‰ ë³€í™”, ì§ˆê°)
- ì˜ˆì‹œ: ë”¸ê¸° + ìš°ìœ  â†’ íë‹¹ ì„ì´ê¸° â†’ ë§ì°¨ í¼ ì˜¬ë¦¬ê¸° â†’ ì™„ì„±ëœ ìŒë£Œ
- í‚¤ì›Œë“œ: "ë§Œë“¤ë‹¤", "ë¶“ë‹¤", "ì„ë‹¤", "craft", "handmade"

**4. Hero's Journey (ì œí’ˆì˜ ì—¬ì •)**
- ì í•©í•œ ì œí’ˆ: ë¸Œëœë“œ ìŠ¤í† ë¦¬ê°€ ê°•í•œ ì œí’ˆ, í”„ë¦¬ë¯¸ì—„ ì œí’ˆ
- êµ¬ì¡°: ì œí’ˆ ì†Œê°œ â†’ íŠ¹ë³„í•œ íŠ¹ì§• â†’ ì œí’ˆì´ ë§Œë“œëŠ” ì„íŒ©íŠ¸
- ì˜ˆì‹œ: ëª…í’ˆ ì‹œê³„ ì†Œê°œ â†’ ì •ë°€í•œ ë¬´ë¸Œë¨¼íŠ¸ â†’ ì‹œê°„ì˜ ê°€ì¹˜

**5. Emotional Arc (ê°ì • ê³¡ì„ )**
- ì í•©í•œ ì œí’ˆ: ëŸ­ì…”ë¦¬, ê°ì„±ì  ì œí’ˆ, ì„ ë¬¼
- êµ¬ì¡°: ê°ì„±ì  Hook â†’ ê°ì • ì—°ê²° â†’ í´ë¼ì´ë§¥ìŠ¤ â†’ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²°ë§
- ì˜ˆì‹œ: íŠ¹ë³„í•œ ìˆœê°„ â†’ ì„ ë¬¼ ë“±ì¥ â†’ ê°ë™ì˜ ìˆœê°„

**6. Lifestyle/Moment (ë¼ì´í”„ìŠ¤íƒ€ì¼ ìˆœê°„)**
- ì í•©í•œ ì œí’ˆ: íŒ¨ì…˜, ì•¡ì„¸ì„œë¦¬, ë¼ì´í”„ìŠ¤íƒ€ì¼ ì œí’ˆ
- êµ¬ì¡°: ì¼ìƒ ì† ìˆœê°„ â†’ ì œí’ˆê³¼ í•¨ê»˜í•˜ëŠ” ëª¨ìŠµ â†’ ì™„ì„±ëœ ë¼ì´í”„ìŠ¤íƒ€ì¼
- ì˜ˆì‹œ: ì¹´í˜ì—ì„œ â†’ ì‹œê³„ ì°©ìš© â†’ ì„¸ë ¨ëœ ì¼ìƒ

**ìŠ¤í† ë¦¬ ë³µì¡ë„ ê°€ì´ë“œ:**
- {cut_count}ê°œ ì»·: {"ê°„ê²°í•˜ê³  ë¹ ë¥¸ ì „ê°œ (í•µì‹¬ ë©”ì‹œì§€ë§Œ)" if cut_count <= 4 else "ì¤‘ê°„ ê¹Šì´ ì „ê°œ (ê°ì • ì—°ê²° + ì œí’ˆ íŠ¹ì§•)" if cut_count <= 6 else "ìƒì„¸í•œ ìŠ¤í† ë¦¬ (ê¹Šì€ ê°ì •ì  ì—¬ì •)"}

**ğŸ‘¥ ìºë¦­í„°/ëª¨ë¸ ê°€ì´ë“œë¼ì¸ (ì‚¬ëŒ ë“±ì¥ ì‹œ í•„ìˆ˜):**

ì‚¬ëŒì´ ë“±ì¥í•˜ëŠ” ì»·ì—ì„œëŠ” ë¸Œëœë“œ ì»¨í…ìŠ¤íŠ¸ì˜ "íƒ€ê²Ÿ ê³ ê°" ì •ë³´ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ê³ :
- **êµ­ì **: ë°˜ë“œì‹œ í•œêµ­ì¸ (Korean)ìœ¼ë¡œ ì„¤ì •
- **ì„±ë³„**: íƒ€ê²Ÿ ì„±ë³„ê³¼ ì¼ì¹˜ (ë‚¨ì„±/ì—¬ì„±/ì¤‘ì„±)
- **ë‚˜ì´ëŒ€**: íƒ€ê²Ÿ ë‚˜ì´ëŒ€ì™€ ì¼ì¹˜ (ì˜ˆ: 20ëŒ€ ì´ˆë°˜, 30ëŒ€, 40-50ëŒ€ ë“±)
- **ì™¸ëª¨ íŠ¹ì§•**: í•œêµ­ì¸ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì™¸ëª¨, í”¼ë¶€í†¤, í—¤ì–´ìŠ¤íƒ€ì¼ í¬í•¨

ì˜ˆì‹œ:
- íƒ€ê²Ÿì´ "20-30ëŒ€ ì—¬ì„±" â†’ image_promptì— "Korean woman in her 20s-30s, natural Korean beauty" í¬í•¨
- íƒ€ê²Ÿì´ "40ëŒ€ ë‚¨ì„±" â†’ image_promptì— "Korean man in his 40s, professional appearance" í¬í•¨

**ğŸ¯ í•µì‹¬ ì›ì¹™: ë¹„ì£¼ì–¼ ì¼ê´€ì„± ë° ì œí’ˆ ì •í™•ì„±**

**ëª¨ë“  ì»· ì´ë¯¸ì§€ëŠ” AIë¡œ 9:16 ë¹„ìœ¨ë¡œ ìƒì„±ë©ë‹ˆë‹¤.**

ì—…ë¡œë“œëœ ì œí’ˆ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬:
1. **ì œí’ˆì˜ ì •í™•í•œ ì™¸ê´€**: í˜•íƒœ, íŒ¨í‚¤ì§•, ë””ìì¸ì„ ì •í™•íˆ ë°˜ì˜
2. **ìƒ‰ìƒ**: ì œí’ˆì˜ ì •í™•í•œ ìƒ‰ìƒ, ìƒ‰ì¡°, ê·¸ë¼ë°ì´ì…˜ ìœ ì§€
3. **ë””í…Œì¼**: ë¡œê³ , ë¼ë²¨, ì§ˆê° ë“± ì„¸ë¶€ ìš”ì†Œ ì¬í˜„
4. **ë¹„ì£¼ì–¼ íŠ¹ì§•**: ì¡°ëª…, ìŠ¤íƒ€ì¼, ë¶„ìœ„ê¸°ë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€

ë¸Œëœë“œ ì»¨í…ìŠ¤íŠ¸ì˜ "ì œí’ˆ ë¹„ì£¼ì–¼ íŠ¹ì§•"ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ê³ , ëª¨ë“  ì»·ì˜ image_promptì— ì´ë¥¼ ë°˜ì˜í•˜ì„¸ìš”.

**ì¤‘ìš”: ë¹„ìš© ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì—¬ ì»· ì •ë³´ì™€ ì „í™˜ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”.**

ê° ìš”ì†Œì˜ êµ¬ì¡°:

**ì»· ì •ë³´:**
1. cut: ì»· ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
2. scene_description: ì¥ë©´ ì„¤ëª… (í•œêµ­ì–´, 2-3ë¬¸ì¥)
3. image_prompt: ì´ë¯¸ì§€ ìƒì„± AI í”„ë¡¬í”„íŠ¸ (ì˜ì–´, ìƒì„¸í•˜ê²Œ)
4. duration: ì»· ê¸¸ì´ (ì´ˆ, **ê³ ì • {cut_duration}ì´ˆ** - í‚¤ í”„ë ˆì„ë§Œ ì§§ê²Œ í‘œì‹œ)
5. is_hero_shot: true/false
   - ì²« ì»·, ë§ˆì§€ë§‰ ì»·, ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ì»·ì€ true
   - ë‚˜ë¨¸ì§€ëŠ” false
6. resolution: "1080p" (hero shot) ë˜ëŠ” "720p" (ì¼ë°˜)
7. needs_text: true/false (í…ìŠ¤íŠ¸ ë Œë”ë§ í•„ìš” ì—¬ë¶€)

   âš ï¸ CRITICAL: needs_textëŠ” ë§¤ìš° ì œí•œì ìœ¼ë¡œë§Œ trueë¡œ ì„¤ì •í•˜ì„¸ìš”.

   **needs_text: trueì¸ ê²½ìš° (ë§¤ìš° ì œí•œì ):**
   - CTA ë©”ì‹œì§€ê°€ í™”ë©´ì— í‘œì‹œë˜ì–´ì•¼ í•˜ëŠ” ê²½ìš°
     * ì˜ˆ: "50% í• ì¸", "ì§€ê¸ˆ êµ¬ë§¤", "NEW", "LIMITED"
   - í•µì‹¬ ì œí’ˆ ì •ë³´ê°€ í…ìŠ¤íŠ¸ë¡œ ëª…í™•íˆ í‘œì‹œë˜ì–´ì•¼ í•˜ëŠ” ê²½ìš°
     * ì˜ˆ: ì˜ì–‘ ì„±ë¶„í‘œì˜ "ì¹¼ë¡œë¦¬ 0", ì„±ë¶„ëª… "ë¹„íƒ€ë¯¼C 500mg"
   - ì¸í¬ê·¸ë˜í”½ ìŠ¤íƒ€ì¼ì˜ í…ìŠ¤íŠ¸ ì„¤ëª…
     * ì˜ˆ: "3ë‹¨ê³„ ê³¼ì •", "Before â†’ After" ë¼ë²¨

   **needs_text: falseì¸ ê²½ìš° (ëŒ€ë¶€ë¶„, ê¸°ë³¸ê°’):**
   - ì œí’ˆ íŒ¨í‚¤ì§€ì˜ ë¸Œëœë“œëª…/ë¡œê³  (AIê°€ ì¬í˜„í•˜ë¯€ë¡œ ë³„ë„ í…ìŠ¤íŠ¸ ë Œë”ë§ ë¶ˆí•„ìš”)
   - ë°°ê²½ì˜ ê°„íŒ, ë©”ë‰´íŒ, í‘œì§€íŒ (ì½ì„ í•„ìš” ì—†ëŠ” ë°°ê²½ ìš”ì†Œ)
   - íë¦¿í•˜ê±°ë‚˜ ì¥ì‹ì ì¸ í…ìŠ¤íŠ¸
   - ìˆœìˆ˜ ë¹„ì£¼ì–¼ ì¥ë©´ (ì‚¬ëŒ, ì œí’ˆ, ì‚¬ìš© ì¥ë©´ ë“±)

   **ì›ì¹™**: "í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì˜ìƒì´ ì„±ë¦½ ì•ˆ ë˜ëŠ” ê²½ìš°"ë§Œ needs_text: true
   **ê¸°ë³¸ê°’**: needs_text: false

**ì „í™˜ ì •ë³´ (ì»·ê³¼ ì»· ì‚¬ì´):**
1. method: "kling" ë˜ëŠ” "ffmpeg"
   - **kling**: ì—­ë™ì  ì›€ì§ì„ ë˜ëŠ” ì‹¤ì œ ì•¡ì…˜ì´ í•„ìš”í•œ ê²½ìš° - Kling 2.1 AI ë¹„ë””ì˜¤ ìƒì„±
     * ì¹´ë©”ë¼ ì›€ì§ì„: ì¤Œì¸/ì•„ì›ƒ, íšŒì „, ë³µì¡í•œ íŒ¨ë‹
     * ê°ì²´ ë™ì‘: íœ˜ì “ê¸°, ë¶“ê¸°, ë“¤ê¸°, ì›€ì§ì´ëŠ” ì†/ì‚¬ëŒ
     * ì—­ë™ì  ì¥ë©´ ì „í™˜: ë¹ ë¥¸ ëª¨ì…˜, ìœ ì²´ ì›€ì§ì„
   - **ffmpeg**: ì •ì  ì¥ë©´ ê°„ ë‹¨ìˆœ ì „í™˜ë§Œ í•„ìš”í•œ ê²½ìš° - ê¸°ë³¸ íš¨ê³¼
     * ë””ì¡¸ë¸Œ, í˜ì´ë“œ, í¬ë¡œìŠ¤í˜ì´ë“œ
     * ë¹„ìŠ·í•œ êµ¬ë„ì˜ ì •ì  ì»· ì‚¬ì´
   - **ì‚¬ìš© ì „ëµ**: ì „ì²´ ì „í™˜ì˜ 50-70%ëŠ” kling ì‚¬ìš© (í€„ë¦¬í‹° ìš°ì„ )
   - **kling ë¹„ìš©**: $0.25/video (5ì´ˆ)
2. effect: ì „í™˜ íš¨ê³¼ëª… (ì°¸ê³ ìš©)
   - kling: "dynamic_zoom_in", "dynamic_zoom_out", "dynamic_pan", "complex_transition"
   - ffmpeg: "dissolve", "fade", "zoom_in", "zoom_out", "pan_left", "pan_right"
3. video_prompt: **êµ¬ì²´ì ì¸ ë¹„ë””ì˜¤ ìƒì„± í”„ë¡¬í”„íŠ¸** (kling ì‚¬ìš© ì‹œ í•„ìˆ˜!)
   - ì œí’ˆ íŠ¹ì§•, ë¸Œëœë“œ í†¤, ì¥ë©´ ì„¤ëª… í¬í•¨
   - ì¹´ë©”ë¼ ì›€ì§ì„, ì¡°ëª…, ë¶„ìœ„ê¸° ìƒì„¸íˆ ê¸°ìˆ 
   - ì• ì»·ê³¼ ë’¤ ì»·ì˜ ì—°ê²°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…
   - ì˜ˆ: "Camera smoothly zooms out from close-up of luxury bottle's golden cap, gradually revealing the full pristine white bottle against minimalist marble background, maintaining soft professional lighting throughout"

   **âš ï¸ ì‚¬ëŒì˜ ë™ì‘ì´ í¬í•¨ë  ë•Œ (í•„ìˆ˜ ì¤€ìˆ˜):**
   - ëª¨ë“  ë™ì‘ì€ **ìì—°ìŠ¤ëŸ½ê³  ì¼ë°˜ì ì¸ ë°©ì‹**ìœ¼ë¡œ í‘œí˜„
   - ê³¼ì¥ë˜ê±°ë‚˜ ì–´ìƒ‰í•œ ë™ì‘ ê¸ˆì§€, ì‹¤ì œ ì‚¬ëŒì´ í•˜ëŠ” ê·¸ëŒ€ë¡œ ë¬˜ì‚¬
   - ì˜ˆì‹œ:
     * ê°€ë°© ë©”ê¸°: "casually swings bag over shoulder in one smooth motion" (ì–´ê¹¨ì— íˆ­ ë©”ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘)
     * ìŒë£Œ ë§ˆì‹œê¸°: "lifts cup naturally to lips, takes a gentle sip"
     * ì œí’ˆ ë“¤ê¸°: "picks up product with relaxed hand movement"
     * ê±·ê¸°: "walks with natural, easy stride"
   - ë™ì‘ì˜ ì†ë„ì™€ ë¦¬ë“¬ì´ í˜„ì‹¤ì ì´ì–´ì•¼ í•¨

   **âš ï¸ ì¬ë£Œ/ì†Œì¬ë¥¼ ë‹¤ë£¨ëŠ” ì¥ë©´ (ë„êµ¬ ì‚¬ìš© í•„ìˆ˜):**
   - ì†ìœ¼ë¡œ ì§ì ‘ ë‹¤ë£¨ê¸° ì–´ë ¤ìš´ ì¬ë£ŒëŠ” **ì ì ˆí•œ ë„êµ¬ë¥¼ ë°˜ë“œì‹œ ëª…ì‹œ**
   - ë„êµ¬ì˜ ì¢…ë¥˜ì™€ ì‚¬ìš© ë°©ë²•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
   - ì˜ˆì‹œ:
     * ë”¸ê¸° í“¨ë ˆ ë‹´ê¸°: "using a long thin spoon to scoop strawberry puree"
     * ë¼ë–¼ ë¶“ê¸°: "pouring latte from stainless steel milk jug with steady hand"
     * ì‹œëŸ½ ë¿Œë¦¬ê¸°: "drizzling syrup using squeeze bottle in circular motion"
     * ê°€ë£¨ ë¿Œë¦¬ê¸°: "using small mesh sifter to dust matcha powder"
     * íœ˜í•‘í¬ë¦¼ ì˜¬ë¦¬ê¸°: "piping whipped cream using pastry bag with star tip"
     * ì¬ë£Œ ì„ê¸°: "stirring ingredients with wooden stirrer/long spoon"
   - ë„êµ¬ ì—†ì´ ì†ìœ¼ë¡œë§Œ í•˜ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì  â†’ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš©

   - ffmpeg ì‚¬ìš© ì‹œì—ëŠ” ê°„ë‹¨íˆ ì‘ì„± (íš¨ê³¼ëª…ë§Œ ì°¸ê³ )
4. duration: ì „í™˜ ê¸¸ì´ (ì´ˆ)
   - **kling**: í‰ê·  {avg_transition_duration:.1f}ì´ˆ (5ì´ˆ ê¶Œì¥)
   - **ffmpeg**: 0.5-2ì´ˆ
5. reason: ì´ ë°©ì‹ì„ ì„ íƒí•œ ì´ìœ  (í•œ ì¤„)

**ìŠ¤í† ë¦¬ë³´ë“œ ì‘ì„± ê°€ì´ë“œë¼ì¸:**
- ì²« ë²ˆì§¸ ì»·: ì„íŒ©íŠ¸ ìˆëŠ” ì˜¤í”„ë‹ (hero shot)
- ì¤‘ê°„ ì»·ë“¤: ì œí’ˆ íŠ¹ì§•, ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤, í˜œíƒ
- ë§ˆì§€ë§‰ ì»·: CTA ë˜ëŠ” ë¸Œëœë“œ ë©”ì‹œì§€ (hero shot)
- **ì „í™˜ ì „ëµ (í€„ë¦¬í‹° ìš°ì„ ):**
  * ì‹¤ì œ ë™ì‘/ì•¡ì…˜ì´ ìˆëŠ” ì¥ë©´ â†’ ë¬´ì¡°ê±´ kling ì‚¬ìš©
  * ì—­ë™ì  ì¹´ë©”ë¼ ì›€ì§ì„ í•„ìš” â†’ kling ì‚¬ìš©
  * ì •ì  ì»· ê°„ ë‹¨ìˆœ ì „í™˜ë§Œ â†’ ffmpeg ì‚¬ìš© ê°€ëŠ¥
  * ì „ì²´ì˜ 50-70%ëŠ” klingìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì˜ìƒì˜ í€˜ë¦¬í‹° í™•ë³´
- **ë™ì‘ ë° ë„êµ¬ ì‚¬ìš© (ì¤‘ìš”!):**
  * ì‚¬ëŒì˜ ëª¨ë“  ë™ì‘ì€ ìì—°ìŠ¤ëŸ½ê³  ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ (ê³¼ì¥ ê¸ˆì§€)
  * ì¬ë£Œ/ì†Œì¬ë¥¼ ë‹¤ë£° ë•ŒëŠ” ì ì ˆí•œ ë„êµ¬ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ (ì†ìœ¼ë¡œë§Œ í•˜ëŠ” ê²ƒ ê¸ˆì§€)
- ì „ì²´ íë¦„ì˜ ë¦¬ë“¬ê° ìœ ì§€
- **image_prompt ì‘ì„± ì‹œ í•„ìˆ˜ ì‚¬í•­:**
  * ë¸Œëœë“œ ì»¨í…ìŠ¤íŠ¸ì˜ "ì œí’ˆ ë¹„ì£¼ì–¼ íŠ¹ì§•"ì— ëª…ì‹œëœ ìƒ‰ìƒ, ìŠ¤íƒ€ì¼, ì¡°ëª…, ë¶„ìœ„ê¸°ë¥¼ **ë°˜ë“œì‹œ** í¬í•¨
  * ì˜ˆ: "ì£¼ìš” ìƒ‰ìƒ: white, gold" â†’ image_promptì— "white and gold color scheme" í¬í•¨
  * ì˜ˆ: "ì¡°ëª… ìŠ¤íƒ€ì¼: soft natural lighting" â†’ ëª¨ë“  ì»·ì— "soft natural lighting" í¬í•¨
  * ì˜ˆ: "ë°°ê²½ ìŠ¤íƒ€ì¼: marble texture" â†’ ë°°ê²½ì´ ìˆëŠ” ì»·ì—ëŠ” "marble background" í¬í•¨
  * ì¡°ëª…, ê°ë„, ë¶„ìœ„ê¸°, ìƒ‰ê°ì„ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ë˜, ì¼ê´€ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ
- video_promptëŠ” ì•ë’¤ ì»·ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì¼ê´€ì„± ìˆê²Œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
  "story_structure": "ì„ íƒí•œ ìŠ¤í† ë¦¬ êµ¬ì¡°ëª… (ì˜ˆ: Process/Creation, Before-After ë“±)",
  "story_rationale": "ì´ ìŠ¤í† ë¦¬ êµ¬ì¡°ë¥¼ ì„ íƒí•œ ì´ìœ  (1-2ë¬¸ì¥, í•œêµ­ì–´)",
  "storyboard": [
    {{
      "cut": 1,
      "scene_description": "[ìŠ¤í† ë¦¬ ì—­í• ] ì¥ë©´ ì„¤ëª…",
      "story_role": "ì´ ì»·ì´ ìŠ¤í† ë¦¬ì—ì„œ ë§¡ëŠ” ì—­í•  (ì˜ˆ: ë¬¸ì œ ì œì‹œ, ì¬ë£Œ ì†Œê°œ ë“±)",
      "image_prompt": "...",
      "duration": {cut_duration},
      "is_hero_shot": true,
      "resolution": "1080p",
      "needs_text": false
    }},
  {{
    "transition": {{
      "method": "kling",
      "effect": "dynamic_zoom_out",
      "video_prompt": "Camera smoothly zooms out from extreme close-up of product detail, gradually revealing full product in elegant setting with professional lighting",
      "duration": {avg_transition_duration:.1f},
      "reason": "ì œí’ˆ ë””í…Œì¼ì—ì„œ ì „ì²´ë¡œ, ê°•ë ¬í•œ ì „í™˜ í•„ìš”"
    }}
  }},
  {{
    "cut": 2,
    "scene_description": "[ìŠ¤í† ë¦¬ ì—­í• ] ì¥ë©´ ì„¤ëª…",
    "story_role": "ì´ ì»·ì´ ìŠ¤í† ë¦¬ì—ì„œ ë§¡ëŠ” ì—­í• ",
    "image_prompt": "...",
    "duration": {cut_duration},
    "is_hero_shot": false,
    "resolution": "720p",
    "needs_text": false
  }},
    ...
  ]
}}

ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ìœ„ í˜•ì‹ì˜ JSONë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

        user_message = f"""ì œí’ˆëª…: {product_name}
ì œí’ˆ ì„¤ëª…: {product_description or 'ì œê³µë˜ì§€ ì•ŠìŒ'}

ë¸Œëœë“œ ì»¨í…ìŠ¤íŠ¸:
{brand_context}

ìœ„ ì œí’ˆ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³ , {cut_count}ê°œì˜ ì»·ìœ¼ë¡œ êµ¬ì„±ëœ ì•½ {duration_seconds}ì´ˆ ê¸¸ì´ì˜ ë§ˆì¼€íŒ… ë¹„ë””ì˜¤ ìŠ¤í† ë¦¬ë³´ë“œë¥¼ JSON ë°°ì—´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."""

        # Vertex AI Gemini API í˜¸ì¶œ
        logger.info(f"Calling Vertex AI Gemini API for storyboard generation ({cut_count} cuts, {duration_seconds}s)")

        # Vertex AI Gemini ëª¨ë¸ ì´ˆê¸°í™”
        gemini_model = VertexGenerativeModel(self.model)

        # image_dataë¥¼ PIL Imageë¡œ ë³€í™˜ í›„ Vertex AI Part ê°ì²´ë¡œ ë³€í™˜
        from PIL import Image
        import io

        image_bytes = base64.b64decode(image_data["data"])
        pil_image = Image.open(io.BytesIO(image_bytes))

        # PIL Image â†’ Vertex AI Part ê°ì²´ ë³€í™˜
        img_byte_arr = io.BytesIO()
        pil_image.save(img_byte_arr, format='JPEG')
        img_bytes = img_byte_arr.getvalue()

        image_part = Part.from_data(
            data=img_bytes,
            mime_type="image/jpeg"
        )

        # System promptì™€ user messageë¥¼ ê²°í•© (GeminiëŠ” system íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›)
        combined_prompt = f"""{system_prompt}

---

{user_message}"""

        # Vertex AI Gemini API í˜¸ì¶œ (Part ê°ì²´ ì‚¬ìš©)
        response = gemini_model.generate_content([combined_prompt, image_part])

        # ì‘ë‹µ íŒŒì‹±
        response_text = response.text
        logger.info(f"Gemini response: {response_text[:200]}...")

        # JSON íŒŒì‹±
        try:
            # JSON ì½”ë“œ ë¸”ë¡ì´ ìˆë‹¤ë©´ ì¶”ì¶œ
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            response_json = json.loads(response_text)

            # ìƒˆ í˜•ì‹ (ê°ì²´) ë˜ëŠ” êµ¬ í˜•ì‹ (ë°°ì—´) ëª¨ë‘ ì§€ì›
            if isinstance(response_json, dict):
                # ìƒˆ í˜•ì‹: {uploaded_image_placement: {...}, storyboard: [...]}
                uploaded_image_placement = response_json.get('uploaded_image_placement', {})
                storyboard = response_json.get('storyboard', [])

                # ë¡œê·¸ ì¶œë ¥
                if uploaded_image_placement:
                    logger.info(f"Uploaded image placement: {uploaded_image_placement.get('position')} (cut {uploaded_image_placement.get('cut_number')})")
                    logger.info(f"Reason: {uploaded_image_placement.get('reason')}")
            elif isinstance(response_json, list):
                # êµ¬ í˜•ì‹: ë°°ì—´ë§Œ
                storyboard = response_json
                uploaded_image_placement = {}
                logger.warning("Old format storyboard (array only)")
            else:
                raise ValueError("Invalid storyboard format")

            # ìœ íš¨ì„± ê²€ì¦
            if not isinstance(storyboard, list):
                raise ValueError("Storyboard must be a list")

            # ì»·ê³¼ ì „í™˜ì„ ë¶„ë¦¬í•˜ì—¬ ê²€ì¦
            cuts = [item for item in storyboard if 'cut' in item]
            transitions = [item for item in storyboard if 'transition' in item]

            if len(cuts) != cut_count:
                logger.warning(f"Expected {cut_count} cuts but got {len(cuts)}")

            # ê° ì»· ê²€ì¦
            for i, cut in enumerate(cuts, 1):
                required_fields = ["cut", "scene_description", "image_prompt", "duration", "is_hero_shot", "resolution"]
                for field in required_fields:
                    if field not in cut:
                        raise ValueError(f"Cut {i} missing required field: {field}")

            # ê° ì „í™˜ ê²€ì¦
            for i, item in enumerate(transitions, 1):
                transition = item.get('transition', {})
                required_fields = ["method", "effect", "duration", "reason"]
                for field in required_fields:
                    if field not in transition:
                        logger.warning(f"Transition {i} missing field: {field}")

            logger.info(f"Storyboard validated: {len(cuts)} cuts, {len(transitions)} transitions")
            return storyboard

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Claude response as JSON: {str(e)}")
            logger.error(f"Response text: {response_text}")
            raise ValueError(f"Invalid JSON response from Claude: {str(e)}")


class ImageGenerationAgent:
    """
    Image Generation Agent
    - Vertex AI Gemini 2.5 Flash Image ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤í† ë¦¬ë³´ë“œ ê° ì»·ì˜ ì´ë¯¸ì§€ ìƒì„±
    - 9:16 ì„¸ë¡œ ë¹„ìœ¨ (ìˆí¼ ìµœì í™”)
    - ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì— PNGë¡œ ì €ì¥
    """

    def __init__(self, model: str = "gemini-2.5-flash-image-ga"):
        self.model = model
        logger.info(f"ImageGenerationAgent initialized with Vertex AI model: {self.model}")

    async def generate_images(
        self,
        job: VideoGenerationJob,
        storyboard: List[Dict[str, Any]],
        db: Session
    ) -> List[Dict[str, str]]:
        """
        ìŠ¤í† ë¦¬ë³´ë“œì˜ ê° ì»·ì— ëŒ€í•œ ì´ë¯¸ì§€ ìƒì„±

        Args:
            job: VideoGenerationJob ì¸ìŠ¤í„´ìŠ¤
            storyboard: ìŠ¤í† ë¦¬ë³´ë“œ ë°ì´í„° (ì»·ê³¼ ì „í™˜ì´ í˜¼í•©ëœ ë°°ì—´)
            db: Database session

        Returns:
            List[Dict]: ìƒì„±ëœ ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
            [{"cut": 1, "url": "https://...", "resolution": "1080p", "is_hero_shot": true}, ...]
        """
        try:
            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ ì»·ë§Œ í•„í„°ë§
            cuts = [item for item in storyboard if 'cut' in item]

            # Job ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = "generating_images"
            job.current_step = f"Generating images for {len(cuts)} cuts"
            db.commit()

            logger.info(f"Starting image generation for job {job.id}: {len(cuts)} cuts")
            logger.info(f"Using Vertex AI Gemini model: {self.model}")

            generated_images = []

            # ë‹¨ì¼ ì´ë¯¸ì§€ ìƒì„± í—¬í¼ í•¨ìˆ˜
            async def generate_single_image(cut, cut_index):
                """ë‹¨ì¼ ì´ë¯¸ì§€ ìƒì„± ë° ì—…ë¡œë“œ"""
                try:
                    cut_number = cut['cut']
                    resolution = cut.get('resolution', '720p')
                    is_hero_shot = cut.get('is_hero_shot', False)
                    needs_text = cut.get('needs_text', False)

                    logger.info(f"Generating image for cut {cut_number}/{len(cuts)}: {cut['image_prompt'][:50]}... (resolution: {resolution}, hero: {is_hero_shot}, needs_text: {needs_text})")

                    # Geminië¡œ ì´ë¯¸ì§€ ìƒì„±
                    image_bytes = await self._generate_with_gemini_image(cut['image_prompt'], needs_text=needs_text)

                    if not image_bytes:
                        raise ValueError(f"Failed to generate image for cut {cut_number}")

                    # ì´ë¯¸ì§€ë¥¼ Supabase Storageì— ì €ì¥
                    image_url = await self._upload_to_supabase(
                        image_bytes,
                        job.user_id,
                        job.id,
                        cut_number,
                        job.session_id
                    )

                    logger.info(f"Image generated and uploaded for cut {cut_number}: {image_url}")

                    return {
                        "cut": cut_number,
                        "url": image_url,
                        "prompt": cut['image_prompt'],
                        "resolution": resolution,
                        "is_hero_shot": is_hero_shot,
                        "source": "generated"
                    }

                except Exception as e:
                    logger.error(f"Error generating image for cut {cut.get('cut', cut_index)}: {str(e)}")
                    return {
                        "cut": cut.get('cut', cut_index),
                        "url": None,
                        "error": str(e),
                        "prompt": cut.get('image_prompt', ''),
                        "resolution": cut.get('resolution', '720p'),
                        "is_hero_shot": cut.get('is_hero_shot', False)
                    }

            # 2ê°œì”© ë³‘ë ¬ ì²˜ë¦¬
            batch_size = 2
            for batch_start in range(0, len(cuts), batch_size):
                batch_end = min(batch_start + batch_size, len(cuts))
                batch_cuts = cuts[batch_start:batch_end]
                batch_num = (batch_start // batch_size) + 1
                total_batches = (len(cuts) + batch_size - 1) // batch_size

                # Job ìƒíƒœ ì—…ë°ì´íŠ¸
                job.current_step = f"Generating images batch {batch_num}/{total_batches} (cuts {batch_start+1}-{batch_end})"
                db.commit()

                logger.info(f"Processing batch {batch_num}/{total_batches}: cuts {batch_start+1}-{batch_end}")

                # ë°°ì¹˜ ë‚´ ì´ë¯¸ì§€ë“¤ì„ ë³‘ë ¬ë¡œ ìƒì„±
                batch_tasks = [
                    generate_single_image(cut, batch_start + i + 1)
                    for i, cut in enumerate(batch_cuts)
                ]
                batch_results = await asyncio.gather(*batch_tasks)
                generated_images.extend(batch_results)

                # ë‹¤ìŒ ë°°ì¹˜ ì „ ì¿¼í„° ì´ˆê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                if batch_end < len(cuts):
                    wait_time = 1  # 1ì´ˆ ëŒ€ê¸° (ê¸°ì¡´ 3ì´ˆì—ì„œ ë‹¨ì¶•)
                    logger.info(f"ë‹¤ìŒ ë°°ì¹˜ ìƒì„± ì „ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘... (ì¿¼í„° ìµœì í™”)")
                    await asyncio.sleep(wait_time)

            # ìƒì„±ëœ ì´ë¯¸ì§€ ì €ì¥
            job.generated_image_urls = generated_images
            db.commit()

            logger.info(f"Image generation completed for job {job.id}: {len([img for img in generated_images if img.get('url')])} successful")
            return generated_images

        except Exception as e:
            logger.error(f"Error in image generation for job {job.id}: {str(e)}")
            job.status = "failed"
            job.error_message = f"Image generation failed: {str(e)}"
            db.commit()
            raise

    async def _generate_with_gemini_image(self, prompt: str, needs_text: bool = False) -> bytes:
        """
        Google Gen AI SDKë¥¼ ì‚¬ìš©í•˜ì—¬ Gemini ì´ë¯¸ì§€ ìƒì„±
        Vertex AI ë°±ì—”ë“œ ì‚¬ìš©, 9:16 aspect ratio ì§€ì›
        Exponential backoff ì¬ì‹œë„ ë¡œì§ í¬í•¨ (429 ì¿¼í„° ì—ëŸ¬ ëŒ€ì‘)

        Args:
            prompt: ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            needs_text: Trueë©´ gemini-3-pro-image-preview (í…ìŠ¤íŠ¸ íŠ¹í™”), Falseë©´ gemini-2.5-flash-image (ì¼ë°˜)
        """
        from google import genai
        from google.genai import types
        from google.api_core.exceptions import ResourceExhausted, TooManyRequests

        # needs_textì— ë”°ë¼ ëª¨ë¸ ë° ë¦¬ì „ ì„ íƒ
        # gemini-3-pro-image-previewëŠ” global ë¦¬ì „ë§Œ ì§€ì›
        model_name = 'gemini-3-pro-image-preview' if needs_text else 'gemini-2.5-flash-image'
        model_display = "Gemini 3 Pro Image (í…ìŠ¤íŠ¸ íŠ¹í™”)" if needs_text else "Gemini 2.5 Flash Image (ì¼ë°˜)"
        model_location = "global" if needs_text else SELECTED_LOCATION

        max_retries = 5
        base_delay = 2  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        for attempt in range(max_retries):
            try:
                logger.info(f"Vertex AI {model_display}ë¡œ ì´ë¯¸ì§€ ìƒì„± ì¤‘ (9:16, location={model_location})... (ì‹œë„ {attempt + 1}/{max_retries}, í”„ë¡¬í”„íŠ¸: {prompt[:50]}...)")

                # Google Gen AI Client ì´ˆê¸°í™” (Vertex AI ë°±ì—”ë“œ ì‚¬ìš©)
                client = genai.Client(
                    vertexai=True,
                    project=os.getenv("GOOGLE_CLOUD_PROJECT"),
                    location=model_location  # needs_text=Trueë©´ global, ì•„ë‹ˆë©´ ê¸°ì¡´ ë¦¬ì „
                )

                # ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ (9:16 aspect ratio ì§€ì •)
                response = client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_modalities=["IMAGE"],
                        image_config=types.ImageConfig(
                            aspect_ratio="9:16",  # ì„¸ë¡œ ë¹„ìœ¨ (short-form video)
                        ),
                    ),
                )

                # ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                for part in response.parts:
                    if part.inline_data:
                        # inline_dataì—ì„œ ì´ë¯¸ì§€ bytes ì¶”ì¶œ
                        if hasattr(part.inline_data, 'data'):
                            image_bytes = part.inline_data.data
                            mime_type = getattr(part.inline_data, 'mime_type', 'image/png')

                            logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ ({model_display}, 9:16 aspect ratio, ì‹œë„ {attempt + 1}, MIME type: {mime_type}, size: {len(image_bytes)} bytes)")
                            return image_bytes

                # ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
                logger.error(f"Gemini ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•¨: {response}")
                raise ValueError("Geminië¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            except Exception as e:
                error_str = str(e)

                # 429 ì¿¼í„° ì—ëŸ¬ì¸ì§€ í™•ì¸ (ë¬¸ìì—´ë¡œ ê²€ì‚¬)
                is_quota_error = "429" in error_str or "RESOURCE_EXHAUSTED" in error_str or "quota" in error_str.lower()

                if is_quota_error:
                    # 429 ì¿¼í„° ì—ëŸ¬ ë°œìƒ ì‹œ exponential backoff ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        wait_time = base_delay * (2 ** attempt)  # 2, 4, 8, 16, 32ì´ˆ
                        logger.warning(f"âš ï¸  429 ì¿¼í„° ì—ëŸ¬ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {error_str[:200]}")
                        logger.info(f"ğŸ”„ {wait_time}ì´ˆ í›„ ì¬ì‹œë„... (exponential backoff)")
                        await asyncio.sleep(wait_time)
                        continue  # ë‹¤ìŒ ì‹œë„ë¡œ
                    else:
                        logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ì— ë„ë‹¬. ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {error_str[:200]}")
                        raise
                else:
                    # 429 ì™¸ì˜ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
                    logger.error(f"âŒ {model_display} ìƒì„± ì‹¤íŒ¨: {error_str}")
                    raise

    async def _upload_to_supabase(
        self,
        image_data: bytes,
        user_id: int,
        job_id: int,
        cut_number: int,
        session_id: str
    ) -> str:
        """ì´ë¯¸ì§€ë¥¼ Supabase Storageì— PNGë¡œ ì €ì¥"""
        try:
            from app.services.supabase_storage import get_storage_service
            storage = get_storage_service()

            # íŒŒì¼ ê²½ë¡œ ìƒì„±
            file_path = f"{user_id}/{session_id}/cut_{cut_number}.png"

            # Supabase Storageì— ì—…ë¡œë“œ
            file_url = storage.upload_file(
                bucket="ai-video-cuts",
                file_path=file_path,
                file_data=image_data,
                content_type="image/png"
            )

            logger.info(f"Image saved to Supabase Storage: {file_url}")
            return file_url
        except Exception as e:
            logger.error(f"Failed to save image to Supabase Storage: {str(e)}")
            raise


class KlingVideoGenerationAgent:
    """
    Kling v2.1 Standard Video Generation Agent
    - fal.ai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ê°„ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„±
    - Image-to-Video ë°©ì‹
    - Front-Last Frame ì§€ì›
    """

    def __init__(self, model: str = "fal-ai/kling-video/v2.1/standard/image-to-video"):
        self.model = model
        self.api_key = os.getenv("FAL_KEY")
        if not self.api_key:
            raise ValueError("FAL_KEY not found in environment variables")
        logger.info(f"KlingVideoGenerationAgent initialized with model: {self.model}")

    def image_to_data_url(self, image_path: str) -> str:
        """
        ë¡œì»¬ ì´ë¯¸ì§€ íŒŒì¼ì„ data URLë¡œ ë³€í™˜

        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

        Returns:
            data URL í˜•ì‹ì˜ ë¬¸ìì—´ (data:image/png;base64,...)
        """
        try:
            # 1. ì´ë¯¸ì§€ ì½ê¸°
            with open(image_path, "rb") as f:
                image_bytes = f.read()

            # 2. base64 ì¸ì½”ë”©
            image_b64 = base64.b64encode(image_bytes).decode("utf-8")

            # 3. MIME type ê²°ì •
            ext = Path(image_path).suffix.lower()
            mime_map = {
                ".png": "image/png",
                ".jpg": "image/jpeg",
                ".jpeg": "image/jpeg",
                ".webp": "image/webp"
            }
            mime_type = mime_map.get(ext, "image/png")

            # 4. data URL ìƒì„±
            data_url = f"data:{mime_type};base64,{image_b64}"
            logger.info(f"Image converted to data URL: {image_path} ({len(data_url)} chars)")
            return data_url

        except Exception as e:
            logger.error(f"Failed to convert image to data URL: {str(e)}")
            raise

    async def download_video(self, video_url: str, save_path: str) -> bool:
        """
        fal.aiì—ì„œ ìƒì„±ëœ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ

        Args:
            video_url: ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ URL
            save_path: ì €ì¥í•  ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                logger.info(f"Downloading video from: {video_url}")
                response = await client.get(video_url)
                response.raise_for_status()

                # ë””ë ‰í† ë¦¬ ìƒì„±
                Path(save_path).parent.mkdir(parents=True, exist_ok=True)

                # íŒŒì¼ ì €ì¥
                with open(save_path, "wb") as f:
                    f.write(response.content)

                file_size_mb = len(response.content) / (1024 * 1024)
                logger.info(f"Video downloaded: {save_path} ({file_size_mb:.2f} MB)")
                return True

        except Exception as e:
            logger.error(f"Download failed: {str(e)}")
            return False

    async def generate_transition_video(
        self,
        start_image_path: str,
        end_image_path: str,
        prompt: str,
        duration: int = 5,
        user_id: int = None,
        job_id: int = None,
        transition_name: str = "transition"
    ) -> dict:
        """
        ë‘ ì´ë¯¸ì§€ ì‚¬ì´ì˜ ì „í™˜ ë¹„ë””ì˜¤ ìƒì„±

        Args:
            start_image_path: ì‹œì‘ ì´ë¯¸ì§€ ê²½ë¡œ
            end_image_path: ì¢…ë£Œ ì´ë¯¸ì§€ ê²½ë¡œ
            prompt: ë¹„ë””ì˜¤ ìƒì„± í”„ë¡¬í”„íŠ¸
            duration: ë¹„ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
            user_id: ì‚¬ìš©ì ID
            job_id: ì‘ì—… ID
            transition_name: ì „í™˜ ì´ë¦„ (ì˜ˆ: "1-2")

        Returns:
            {
                "transition": "1-2",
                "url": "/uploads/...",
                "method": "kling",
                "effect": "...",
                "error": None
            }
        """
        try:
            logger.info(f"Generating Kling video for {transition_name}: {prompt}")

            # 1. ì‹œì‘ ì´ë¯¸ì§€ë¥¼ data URLë¡œ ë³€í™˜ (Klingì€ ì‹œì‘ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©)
            image_data_url = self.image_to_data_url(start_image_path)

            # 2. fal.ai API í˜¸ì¶œ
            logger.info(f"Calling fal.ai API...")

            # asyncio.to_threadë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            result = await asyncio.to_thread(
                fal_client.subscribe,
                self.model,
                arguments={
                    "image_url": image_data_url,
                    "prompt": prompt,
                    "duration": duration,
                    "aspect_ratio": "9:16"
                },
                with_logs=False
            )

            logger.info(f"Kling API response received")

            # 3. ì‘ë‹µì—ì„œ ë¹„ë””ì˜¤ URL ì¶”ì¶œ
            if not result or "video" not in result:
                raise ValueError(f"Invalid API response: {result}")

            video_url = result["video"]["url"]
            logger.info(f"Video URL: {video_url}")

            # 4. ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ë° Supabase Storageì— ì—…ë¡œë“œ
            # ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œ
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get(video_url) as response:
                    if response.status != 200:
                        raise Exception(f"Failed to download video: HTTP {response.status}")
                    video_bytes = await response.read()

            # Supabase Storageì— ì—…ë¡œë“œ
            from app.services.supabase_storage import get_storage_service
            from app.database import SessionLocal

            # session_id ê°€ì ¸ì˜¤ê¸°
            db = SessionLocal()
            try:
                from app import models
                job = db.query(models.VideoGenerationJob).filter(
                    models.VideoGenerationJob.id == job_id
                ).first()
                session_id = job.session_id if job else str(job_id)
            finally:
                db.close()

            storage = get_storage_service()
            file_path = f"{user_id}/{session_id}/{transition_name}.mp4"

            relative_url = storage.upload_file(
                bucket="ai-video-transitions",
                file_path=file_path,
                file_data=video_bytes,
                content_type="video/mp4"
            )

            logger.info(f"Kling video generated successfully: {relative_url}")

            return {
                "success": True,
                "transition": transition_name,
                "url": relative_url,
                "method": "kling",
                "effect": prompt,
                "error": None,
                "cost": 0.25  # USD
            }

        except Exception as e:
            logger.error(f"Failed to generate Kling video: {str(e)}")
            return {
                "success": False,
                "transition": transition_name,
                "url": None,
                "method": "kling",
                "effect": prompt,
                "error": str(e),
                "cost": 0
            }

    async def generate_transition_videos(
        self,
        job: VideoGenerationJob,
        storyboard: List[Dict[str, Any]],
        images: List[Dict[str, str]],
        db: Session
    ) -> List[Dict[str, str]]:
        """
        ì´ë¯¸ì§€ ê°„ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„± (Kling ë°©ì‹ë§Œ ì„ íƒì ìœ¼ë¡œ)

        Args:
            job: VideoGenerationJob ì¸ìŠ¤í„´ìŠ¤
            storyboard: ìŠ¤í† ë¦¬ë³´ë“œ ë°ì´í„° (ì „í™˜ ì •ë³´ í¬í•¨)
            images: ìƒì„±ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            db: Database session

        Returns:
            List[Dict]: ìƒì„±ëœ ë¹„ë””ì˜¤ URL ë¦¬ìŠ¤íŠ¸ (Kling ì „í™˜ë§Œ)
        """
        try:
            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ Kling ë°©ì‹ì˜ ì „í™˜ë§Œ í•„í„°ë§
            kling_transitions = [
                item['transition'] for item in storyboard
                if 'transition' in item and item['transition'].get('method') == 'kling'
            ]

            # Job ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = "generating_videos"
            job.current_step = f"Generating {len(kling_transitions)} Kling transition videos"
            db.commit()

            logger.info(f"Starting Kling video generation for job {job.id}: {len(kling_transitions)} transitions (FFmpeg transitions will be handled in composition)")

            if not kling_transitions:
                logger.info("No Kling transitions needed - all transitions will use FFmpeg")
                job.generated_video_urls = []
                db.commit()
                return []

            generated_videos = []

            # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
            valid_images = [img for img in images if img.get('url')]

            if len(valid_images) < 2:
                raise ValueError("Need at least 2 images to create transition videos")

            # ì´ë¯¸ì§€ë¥¼ cut ë²ˆí˜¸ë¡œ ë§¤í•‘
            image_by_cut = {img['cut']: img for img in valid_images}

            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ ì „í™˜ê³¼ ì»·ì˜ ë§¤í•‘ ìƒì„±
            cuts = [item for item in storyboard if 'cut' in item]

            # ê° Kling ì „í™˜ ë¹„ë””ì˜¤ ìƒì„±
            for idx, transition_data in enumerate(kling_transitions, 1):
                try:
                    effect = transition_data.get('effect', 'smooth_transition')
                    duration = transition_data.get('duration', 5.0)
                    reason = transition_data.get('reason', '')

                    # Master Agentê°€ ìƒì„±í•œ video_prompt ì‚¬ìš©
                    video_prompt = transition_data.get('video_prompt',
                        'Smooth, cinematic transition from the first image to the second image.')

                    # ì „í™˜ì´ ì–´ëŠ ì»· ì‚¬ì´ì¸ì§€ ì¶”ë¡  (ìŠ¤í† ë¦¬ë³´ë“œì˜ ìˆœì„œ ê¸°ë°˜)
                    transition_index = None
                    for i, item in enumerate(storyboard):
                        if 'transition' in item and item['transition'] == transition_data:
                            transition_index = i
                            break

                    if transition_index is None:
                        logger.warning(f"Could not find transition in storyboard, skipping")
                        continue

                    # ì•ë’¤ ì»· ì°¾ê¸°
                    from_cut = None
                    to_cut = None
                    for i in range(transition_index - 1, -1, -1):
                        if 'cut' in storyboard[i]:
                            from_cut = storyboard[i]['cut']
                            break
                    for i in range(transition_index + 1, len(storyboard)):
                        if 'cut' in storyboard[i]:
                            to_cut = storyboard[i]['cut']
                            break

                    if not from_cut or not to_cut:
                        logger.warning(f"Could not determine from/to cuts for transition, skipping")
                        continue

                    from_image = image_by_cut.get(from_cut)
                    to_image = image_by_cut.get(to_cut)

                    if not from_image or not to_image:
                        logger.warning(f"Missing images for transition {from_cut}-{to_cut}, skipping")
                        continue

                    transition_name = f"{from_cut}-{to_cut}"

                    logger.info(f"Generating Kling transition video {idx}/{len(kling_transitions)}: {transition_name} (prompt: {video_prompt[:80]}...)")

                    # Job ìƒíƒœ ì—…ë°ì´íŠ¸
                    job.current_step = f"Generating Kling transition {idx}/{len(kling_transitions)}"
                    db.commit()

                    # ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                    from_image_path = from_image['url']  # ì˜ˆ: "/uploads/ai_video_images/1/18/cut_1.png"
                    to_image_path = to_image['url']

                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    from_image_abs = Path(__file__).parent.parent.parent / from_image_path.lstrip('/')
                    to_image_abs = Path(__file__).parent.parent.parent / to_image_path.lstrip('/')

                    # Kling API í˜¸ì¶œ
                    result = await self.generate_transition_video(
                        start_image_path=str(from_image_abs),
                        end_image_path=str(to_image_abs),
                        prompt=video_prompt,
                        duration=int(duration),
                        user_id=job.user_id,
                        job_id=job.id,
                        transition_name=transition_name
                    )

                    if result.get('success'):
                        generated_videos.append({
                            "transition": transition_name,
                            "url": result['url'],
                            "from_cut": from_cut,
                            "to_cut": to_cut,
                            "method": "kling",
                            "effect": effect,
                            "duration": duration,
                            "reason": reason,
                            "cost": result.get('cost', 0.25)
                        })
                        logger.info(f"Kling transition video generated: {transition_name} -> {result['url']}")
                    else:
                        logger.error(f"Failed to generate Kling video for {transition_name}: {result.get('error')}")
                        generated_videos.append({
                            "transition": transition_name,
                            "url": None,
                            "error": result.get('error'),
                            "method": "kling",
                            "effect": effect
                        })

                except Exception as e:
                    logger.error(f"Error generating Kling transition video: {str(e)}")
                    if 'transition_name' in locals():
                        generated_videos.append({
                            "transition": transition_name,
                            "url": None,
                            "error": str(e),
                            "method": "kling",
                            "effect": transition_data.get('effect', '')
                        })

            # ìƒì„±ëœ ë¹„ë””ì˜¤ ì €ì¥
            job.generated_video_urls = generated_videos
            db.commit()

            logger.info(f"Kling video generation completed for job {job.id}: {len([vid for vid in generated_videos if vid.get('url')])} successful")
            return generated_videos

        except Exception as e:
            logger.error(f"Error in video generation for job {job.id}: {str(e)}")
            job.status = "failed"
            job.error_message = f"Video generation failed: {str(e)}"
            db.commit()
            raise


class VideoGenerationAgent:
    """
    Video Generation Agent
    - Vertex AI Veo 3.1ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ê°„ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„±
    - ìƒì„±ëœ ë¹„ë””ì˜¤ë¥¼ Cloudinaryì— ì—…ë¡œë“œ
    """

    def __init__(self, model: str = "veo-3.1-fast-generate-001"):
        self.model = model
        logger.info(f"VideoGenerationAgent initialized with Vertex AI model: {self.model}")

    async def generate_transition_videos(
        self,
        job: VideoGenerationJob,
        storyboard: List[Dict[str, Any]],
        images: List[Dict[str, str]],
        db: Session
    ) -> List[Dict[str, str]]:
        """
        ì´ë¯¸ì§€ ê°„ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„± (Veo ë°©ì‹ë§Œ ì„ íƒì ìœ¼ë¡œ)

        Args:
            job: VideoGenerationJob ì¸ìŠ¤í„´ìŠ¤
            storyboard: ìŠ¤í† ë¦¬ë³´ë“œ ë°ì´í„° (ì „í™˜ ì •ë³´ í¬í•¨)
            images: ìƒì„±ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            db: Database session

        Returns:
            List[Dict]: ìƒì„±ëœ ë¹„ë””ì˜¤ URL ë¦¬ìŠ¤íŠ¸ (Veo ì „í™˜ë§Œ)
            [{"transition": "1-2", "url": "https://...", "method": "veo", "effect": "..."}, ...]
        """
        try:
            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ Veo ë°©ì‹ì˜ ì „í™˜ë§Œ í•„í„°ë§
            veo_transitions = [
                item['transition'] for item in storyboard
                if 'transition' in item and item['transition'].get('method') == 'veo'
            ]

            # Job ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = "generating_videos"
            job.current_step = f"Generating {len(veo_transitions)} Veo transition videos"
            db.commit()

            logger.info(f"Starting Veo video generation for job {job.id}: {len(veo_transitions)} transitions (FFmpeg transitions will be handled in composition)")

            if not veo_transitions:
                logger.info("No Veo transitions needed - all transitions will use FFmpeg")
                job.generated_video_urls = []
                db.commit()
                return []

            generated_videos = []

            # Vertex AI Veo ëª¨ë¸ ì´ˆê¸°í™”
            try:
                veo_model = VertexGenerativeModel(self.model)
                logger.info(f"âœ… Using Vertex AI Veo model: {self.model}")
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Veo model: {str(e)}")
                raise ValueError(f"Failed to initialize Veo model '{self.model}': {str(e)}")

            # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
            valid_images = [img for img in images if img.get('url')]

            if len(valid_images) < 2:
                raise ValueError("Need at least 2 images to create transition videos")

            # ì´ë¯¸ì§€ë¥¼ cut ë²ˆí˜¸ë¡œ ë§¤í•‘
            image_by_cut = {img['cut']: img for img in valid_images}

            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ ì „í™˜ê³¼ ì»·ì˜ ë§¤í•‘ ìƒì„±
            cuts = [item for item in storyboard if 'cut' in item]

            # ê° Veo ì „í™˜ ë¹„ë””ì˜¤ ìƒì„±
            for idx, transition_data in enumerate(veo_transitions, 1):
                try:
                    effect = transition_data.get('effect', 'smooth_transition')
                    duration = transition_data.get('duration', 4.0)
                    reason = transition_data.get('reason', '')

                    # ì „í™˜ì´ ì–´ëŠ ì»· ì‚¬ì´ì¸ì§€ ì¶”ë¡  (ìŠ¤í† ë¦¬ë³´ë“œì˜ ìˆœì„œ ê¸°ë°˜)
                    # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ ì „í™˜ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ ì•ë’¤ ì»· ë²ˆí˜¸ ì¶”ì¶œ
                    transition_index = None
                    for i, item in enumerate(storyboard):
                        if 'transition' in item and item['transition'] == transition_data:
                            transition_index = i
                            break

                    if transition_index is None:
                        logger.warning(f"Could not find transition in storyboard, skipping")
                        continue

                    # ì•ë’¤ ì»· ì°¾ê¸°
                    from_cut = None
                    to_cut = None
                    for i in range(transition_index - 1, -1, -1):
                        if 'cut' in storyboard[i]:
                            from_cut = storyboard[i]['cut']
                            break
                    for i in range(transition_index + 1, len(storyboard)):
                        if 'cut' in storyboard[i]:
                            to_cut = storyboard[i]['cut']
                            break

                    if not from_cut or not to_cut:
                        logger.warning(f"Could not determine from/to cuts for transition, skipping")
                        continue

                    from_image = image_by_cut.get(from_cut)
                    to_image = image_by_cut.get(to_cut)

                    if not from_image or not to_image:
                        logger.warning(f"Missing images for transition {from_cut}-{to_cut}, skipping")
                        continue

                    transition_name = f"{from_cut}-{to_cut}"

                    logger.info(f"Generating Veo transition video {idx}/{len(veo_transitions)}: {transition_name} (effect: {effect})")

                    # Job ìƒíƒœ ì—…ë°ì´íŠ¸
                    job.current_step = f"Generating Veo transition {idx}/{len(veo_transitions)} ({effect})"
                    db.commit()

                    # íš¨ê³¼ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    video_prompt = self._create_veo_prompt(effect, duration)
                    logger.info(f"Video prompt: {video_prompt}")

                    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° base64 ì¸ì½”ë”©
                    from_image_data = await self._download_image(from_image['url'])

                    logger.info(f"Downloaded from_image: {len(from_image_data)} bytes")

                    # PILë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬
                    from PIL import Image
                    import io

                    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ reference imageë¡œ ì‚¬ìš©
                    pil_image = Image.open(io.BytesIO(from_image_data))

                    # PIL Image â†’ bytes ë³€í™˜
                    img_byte_arr = io.BytesIO()
                    pil_image.save(img_byte_arr, format='PNG')
                    reference_image_bytes = img_byte_arr.getvalue()

                    # Veo 3.1 API í˜¸ì¶œ (with exponential backoff for rate limiting)
                    # ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ ìƒì„±
                    logger.info(f"Calling Veo API with prompt: {video_prompt}")

                    # Exponential backoff ì„¤ì •
                    max_retries = 5
                    base_delay = 2  # seconds
                    response = None

                    for attempt in range(max_retries):
                        try:
                            response = veo_model.generate_content([
                                Part.from_data(data=reference_image_bytes, mime_type="image/png"),
                                f"{video_prompt}. Duration: {int(duration)} seconds. Aspect ratio: 9:16 vertical video for social media."
                            ])

                            # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
                            logger.info(f"Veo API call successful (attempt {attempt + 1}/{max_retries})")
                            break

                        except Exception as api_error:
                            error_str = str(api_error)

                            # 429 (Resource Exhausted) ë˜ëŠ” 503 (Service Unavailable) ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                            if '429' in error_str or 'ResourceExhausted' in error_str or '503' in error_str:
                                if attempt < max_retries - 1:
                                    delay = base_delay * (2 ** attempt)  # exponential backoff: 2s, 4s, 8s, 16s, 32s
                                    logger.warning(f"Rate limit/throttling detected (attempt {attempt + 1}/{max_retries}), retrying in {delay}s...")
                                    await asyncio.sleep(delay)
                                else:
                                    logger.error(f"Max retries ({max_retries}) reached for {transition_name}, giving up")
                                    raise
                            else:
                                # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°”ë¡œ raise
                                raise

                    # ë¹„ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ
                    if not response or not response.candidates:
                        raise ValueError(f"No response from Veo API for transition {transition_name}")

                    # ì‘ë‹µ êµ¬ì¡° í™•ì¸
                    logger.info(f"Veo response type: {type(response)}")

                    # ë¹„ë””ì˜¤ ë°ì´í„° ì°¾ê¸°
                    video_data = None
                    for part in response.candidates[0].content.parts:
                        if hasattr(part, 'inline_data') and part.inline_data:
                            # MIME íƒ€ì…ì´ videoì¸ì§€ í™•ì¸
                            if 'video' in part.inline_data.mime_type:
                                video_data = part.inline_data.data
                                logger.info(f"Found video data: mime_type={part.inline_data.mime_type}")
                                break

                    if not video_data:
                        raise ValueError(f"No video data in Veo response for transition {transition_name}")

                    # ë¹„ë””ì˜¤ ì €ì¥ (Supabase Storage)
                    from app.services.supabase_storage import get_storage_service
                    storage = get_storage_service()

                    # base64 ë””ì½”ë”©
                    video_bytes = base64.b64decode(video_data)

                    # Supabase Storageì— ì—…ë¡œë“œ
                    file_path = f"{job.user_id}/{job.session_id}/{transition_name}.mp4"
                    video_url = storage.upload_file(
                        bucket="ai-video-transitions",
                        file_path=file_path,
                        file_data=video_bytes,
                        content_type="video/mp4"
                    )

                    logger.info(f"Veo video saved to Supabase Storage: {video_url} ({len(video_bytes)} bytes)")

                    generated_videos.append({
                        "transition": transition_name,
                        "url": video_url,
                        "from_cut": from_cut,
                        "to_cut": to_cut,
                        "method": "veo",
                        "effect": effect,
                        "duration": duration,
                        "reason": reason
                    })

                    logger.info(f"Veo transition video generated and uploaded: {transition_name} -> {video_url}")

                except Exception as e:
                    logger.error(f"Error generating Veo transition video: {str(e)}")
                    # ì¼ë¶€ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    if 'transition_name' in locals():
                        generated_videos.append({
                            "transition": transition_name,
                            "url": None,
                            "error": str(e),
                            "method": "veo",
                            "effect": transition_data.get('effect', '')
                        })

            # ìƒì„±ëœ ë¹„ë””ì˜¤ ì €ì¥
            job.generated_video_urls = generated_videos
            db.commit()

            logger.info(f"Veo video generation completed for job {job.id}: {len([vid for vid in generated_videos if vid.get('url')])} successful")
            return generated_videos

        except Exception as e:
            logger.error(f"Error in video generation for job {job.id}: {str(e)}")
            job.status = "failed"
            job.error_message = f"Video generation failed: {str(e)}"
            db.commit()
            raise

    def _create_veo_prompt(self, effect: str, duration: float) -> str:
        """íš¨ê³¼ì— ë”°ë¥¸ Veo í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = {
            "dynamic_zoom_in": "Smooth, cinematic zoom in from the first image to the second image. Professional camera movement with elegant transition.",
            "dynamic_zoom_out": "Smooth, cinematic zoom out from the first image to the second image. Professional camera movement with elegant transition.",
            "dynamic_pan": "Smooth, cinematic panning from the first image to the second image. Professional lateral camera movement.",
            "complex_transition": "Dynamic, creative transition from the first image to the second image. Cinematic and engaging camera movement."
        }
        return prompts.get(effect, "Smooth transition from the first image to the second image. Professional, cinematic camera movement.")

    async def _download_image(self, url: str) -> bytes:
        """
        ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (Supabase Storage URLì—ì„œ HTTP ë‹¤ìš´ë¡œë“œ)
        """
        try:
            logger.info(f"Downloading image from Supabase Storage: {url}")
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                response.raise_for_status()
                logger.info(f"Successfully downloaded image: {len(response.content)} bytes")
                return response.content
        except Exception as e:
            logger.error(f"Failed to download image from {url}: {str(e)}")
            raise

    async def _save_transition_to_local(
        self,
        video_data: bytes,
        user_id: int,
        job_id: int,
        transition_name: str
    ) -> str:
        """ì „í™˜ ë¹„ë””ì˜¤ë¥¼ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì— ì„ì‹œ ì €ì¥"""
        try:
            # ì €ì¥ ê²½ë¡œ ìƒì„±
            save_dir = Path("uploads") / "ai_video_transitions" / str(user_id) / str(job_id)
            save_dir.mkdir(parents=True, exist_ok=True)

            # íŒŒì¼ ì €ì¥
            file_path = save_dir / f"transition_{transition_name}.mp4"
            with open(file_path, 'wb') as f:
                f.write(video_data)

            # URL ë°˜í™˜ (FastAPI static files ê²½ë¡œ)
            file_url = f"/uploads/ai_video_transitions/{user_id}/{job_id}/transition_{transition_name}.mp4"
            logger.info(f"Video saved to local filesystem: {file_url}")
            return file_url
        except Exception as e:
            logger.error(f"Failed to save video to local filesystem: {str(e)}")
            raise


class KlingVideoGenerationAgent:
    """
    Kling 2.1 Standard Video Generation Agent (via fal.ai)
    - fal.ai APIë¥¼ í†µí•´ Kling 2.1 Standard ëª¨ë¸ë¡œ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„±
    - Image-to-Video ë°©ì‹ (First Frame â†’ Motion â†’ Last Frame)
    - 9:16 ì„¸ë¡œ ë¹„ìœ¨ ë„¤ì´í‹°ë¸Œ ì§€ì›
    - ìƒì„± ì‹œê°„: 5ì´ˆ ì˜ìƒ ì•½ 30ì´ˆ (MiniMax ëŒ€ë¹„ 33% ë‹¨ì¶•)
    - ë¹„ìš©: $0.25/5ì´ˆ (MiniMax ëŒ€ë¹„ 10% ì ˆê°)
    """

    def __init__(
        self,
        duration: str = "5",  # "5" or "10" seconds
        aspect_ratio: str = "9:16"
    ):
        self.duration = duration
        self.aspect_ratio = aspect_ratio
        self.api_key = os.getenv("FAL_KEY")
        self.model_id = "fal-ai/kling-video/v2.1/standard/image-to-video"

        if not self.api_key:
            raise ValueError("FAL_KEY not found in environment variables")

        # API Key ë””ë²„ê¹… (ì²˜ìŒ 10ìë§Œ í‘œì‹œ)
        api_key_preview = self.api_key[:10] + "..." if len(self.api_key) > 10 else self.api_key
        logger.info(f"KlingVideoGenerationAgent initialized: model=Kling 2.1 Standard, duration={self.duration}s, aspect_ratio={self.aspect_ratio}")
        logger.info(f"FAL API Key loaded: {api_key_preview} (length: {len(self.api_key)})")

    async def generate_transition_videos(
        self,
        job: VideoGenerationJob,
        storyboard: List[Dict[str, Any]],
        images: List[Dict[str, str]],
        db: Session
    ) -> List[Dict[str, str]]:
        """
        ì´ë¯¸ì§€ ê°„ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„± (Kling 2.1 via fal.ai)
        """
        try:
            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ AI ë¹„ë””ì˜¤ ë°©ì‹ì˜ ì „í™˜ë§Œ í•„í„°ë§ (minimax â†’ klingìœ¼ë¡œ ì²˜ë¦¬)
            transitions = [
                item['transition'] for item in storyboard
                if 'transition' in item and item['transition'].get('method') in ['minimax', 'kling']
            ]

            # Job ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = "generating_videos"
            job.current_step = f"Generating {len(transitions)} Kling 2.1 transition videos"
            db.commit()

            logger.info(f"Starting Kling 2.1 video generation for job {job.id}: {len(transitions)} transitions")

            if not transitions:
                logger.info("No transitions needed - all transitions will use FFmpeg")
                job.generated_video_urls = []
                db.commit()
                return []

            generated_videos = []

            # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
            valid_images = [img for img in images if img.get('url')]

            if len(valid_images) < 2:
                raise ValueError("Need at least 2 images to create transition videos")

            # ì´ë¯¸ì§€ë¥¼ cut ë²ˆí˜¸ë¡œ ë§¤í•‘
            image_by_cut = {img['cut']: img for img in valid_images}

            # ì „í™˜ ì •ë³´ ì¤€ë¹„ (from_cut, to_cut, ê¸°íƒ€ ì •ë³´ ì¶”ì¶œ)
            transition_tasks = []
            for idx, transition_data in enumerate(transitions, 1):
                video_prompt = transition_data.get('video_prompt', '')
                effect = transition_data.get('effect', 'smooth_transition')
                duration = transition_data.get('duration', 5)
                reason = transition_data.get('reason', '')

                # ì „í™˜ì´ ì–´ëŠ ì»· ì‚¬ì´ì¸ì§€ ì¶”ë¡ 
                transition_index = None
                for i, item in enumerate(storyboard):
                    if 'transition' in item and item['transition'] == transition_data:
                        transition_index = i
                        break

                if transition_index is None:
                    logger.warning(f"Could not find transition in storyboard, skipping")
                    continue

                # ì•ë’¤ ì»· ì°¾ê¸°
                from_cut = None
                to_cut = None

                for i in range(transition_index - 1, -1, -1):
                    if 'cut' in storyboard[i]:
                        from_cut = storyboard[i]['cut']
                        break

                for i in range(transition_index + 1, len(storyboard)):
                    if 'cut' in storyboard[i]:
                        to_cut = storyboard[i]['cut']
                        break

                if not from_cut or not to_cut:
                    logger.warning(f"Could not determine from/to cuts, skipping transition")
                    continue

                if from_cut not in image_by_cut or to_cut not in image_by_cut:
                    logger.warning(f"Images for transition {from_cut}-{to_cut} not found, skipping")
                    continue

                transition_name = f"{from_cut}-{to_cut}"
                start_image_url = image_by_cut[from_cut]['url']

                transition_tasks.append({
                    "idx": idx,
                    "transition_name": transition_name,
                    "from_cut": from_cut,
                    "to_cut": to_cut,
                    "image_url": start_image_url,
                    "video_prompt": video_prompt,
                    "effect": effect,
                    "duration": duration,
                    "reason": reason
                })

            # 2ê°œì”© ë°°ì¹˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (fal.ai ë™ì‹œ 2ê°œ ì œí•œ)
            batch_size = 2
            total_tasks = len(transition_tasks)

            async def generate_single_video(task_info):
                """ë‹¨ì¼ ì „í™˜ ì˜ìƒ ìƒì„±"""
                try:
                    video_url = await self._generate_video_with_retry(
                        image_url=task_info["image_url"],
                        prompt=task_info["video_prompt"],
                        job=job,
                        transition_name=task_info["transition_name"]
                    )

                    cost = 0.25 if self.duration == "5" else 0.50

                    return {
                        "transition": task_info["transition_name"],
                        "url": video_url,
                        "from_cut": task_info["from_cut"],
                        "to_cut": task_info["to_cut"],
                        "method": "kling",
                        "effect": task_info["effect"],
                        "duration": int(self.duration),
                        "reason": task_info["reason"],
                        "cost": cost
                    }
                except Exception as e:
                    logger.error(f"Error generating Kling 2.1 video {task_info['transition_name']}: {str(e)}")
                    return {
                        "transition": task_info["transition_name"],
                        "url": None,
                        "error": str(e),
                        "method": "kling",
                        "effect": task_info["effect"]
                    }

            for batch_start in range(0, total_tasks, batch_size):
                batch_end = min(batch_start + batch_size, total_tasks)
                batch = transition_tasks[batch_start:batch_end]
                batch_num = (batch_start // batch_size) + 1
                total_batches = (total_tasks + batch_size - 1) // batch_size

                logger.info(f"Processing Kling 2.1 batch {batch_num}/{total_batches}: {len(batch)} videos")
                job.current_step = f"Generating Kling 2.1 transitions batch {batch_num}/{total_batches}"
                db.commit()

                # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
                batch_results = await asyncio.gather(*[
                    generate_single_video(task) for task in batch
                ])
                generated_videos.extend(batch_results)

                logger.info(f"Batch {batch_num}/{total_batches} completed")

            # ìƒì„±ëœ ë¹„ë””ì˜¤ ì €ì¥
            job.generated_video_urls = generated_videos
            db.commit()

            logger.info(f"Kling 2.1 video generation completed for job {job.id}: {len([v for v in generated_videos if v.get('url')])} successful")
            return generated_videos

        except Exception as e:
            logger.error(f"Error in Kling 2.1 video generation for job {job.id}: {str(e)}")
            job.status = "failed"
            job.error_message = f"Kling 2.1 video generation failed: {str(e)}"
            db.commit()
            raise

    async def _generate_video_with_retry(
        self,
        image_url: str,
        prompt: str,
        job: VideoGenerationJob,
        transition_name: str,
        max_retries: int = 3
    ) -> str:
        """
        fal.ai Kling API í˜¸ì¶œ with retry
        """
        import fal_client

        base_delay = 2

        for attempt in range(max_retries):
            try:
                logger.info(f"Calling fal.ai Kling 2.1 API (attempt {attempt + 1}/{max_retries})")

                # fal.ai subscribe (ë™ê¸° ëŒ€ê¸°)
                result = await asyncio.to_thread(
                    fal_client.subscribe,
                    self.model_id,
                    arguments={
                        "prompt": prompt,
                        "image_url": image_url,
                        "duration": self.duration,
                        "aspect_ratio": self.aspect_ratio
                    }
                )

                # ê²°ê³¼ì—ì„œ ë¹„ë””ì˜¤ URL ì¶”ì¶œ
                video_data = result.get("video", {})
                fal_video_url = video_data.get("url")

                if not fal_video_url:
                    raise Exception(f"No video URL in fal.ai response: {result}")

                logger.info(f"Kling 2.1 video generated via fal.ai: {fal_video_url}")

                # ë¹„ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Supabase Storageì— ì €ì¥
                video_url = await self._download_and_store_video(
                    fal_video_url=fal_video_url,
                    job=job,
                    transition_name=transition_name
                )

                return video_url

            except Exception as e:
                error_str = str(e)

                # Rate limit ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                if '429' in error_str or 'rate limit' in error_str.lower() or 'quota' in error_str.lower():
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)
                        logger.warning(f"Rate limit detected (attempt {attempt + 1}/{max_retries}), retrying in {delay}s...")
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"Max retries ({max_retries}) reached, giving up")
                        raise
                else:
                    raise

    async def _download_and_store_video(
        self,
        fal_video_url: str,
        job: VideoGenerationJob,
        transition_name: str
    ) -> str:
        """
        fal.aiì—ì„œ ìƒì„±ëœ ë¹„ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Supabase Storageì— ì €ì¥
        """
        # ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.get(fal_video_url)

            if response.status_code != 200:
                raise Exception(f"Failed to download video from fal.ai: {response.status_code}")

            video_bytes = response.content

        # Supabase Storageì— ì €ì¥
        from app.services.supabase_storage import get_storage_service
        storage = get_storage_service()

        file_path = f"{job.user_id}/{job.session_id}/{transition_name}.mp4"

        video_url = storage.upload_file(
            bucket="ai-video-transitions",
            file_path=file_path,
            file_data=video_bytes,
            content_type="video/mp4"
        )

        logger.info(f"Kling video saved to Supabase Storage: {video_url} ({len(video_bytes)} bytes)")

        return video_url


class VideoCompositionAgent:
    """
    Video Composition Agent
    - moviepy/ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ë¥¼ ìµœì¢… ë¹„ë””ì˜¤ë¡œ í•©ì„±
    - AI ë¹„ë””ì˜¤ (Kling) + FFmpeg ê¸°ë°˜ ê°„ë‹¨í•œ ì „í™˜ íš¨ê³¼ í˜¼í•© ì§€ì›
    - ìƒì„±ëœ ìµœì¢… ë¹„ë””ì˜¤ë¥¼ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
    """

    def __init__(self):
        pass

    def _create_ffmpeg_transition(
        self,
        from_clip,
        to_clip,
        effect: str,
        duration: float
    ):
        """
        FFmpeg ê¸°ë°˜ ì „í™˜ íš¨ê³¼ ìƒì„±

        Args:
            from_clip: ì‹œì‘ í´ë¦½
            to_clip: ì¢…ë£Œ í´ë¦½
            effect: ì „í™˜ íš¨ê³¼ëª… (dissolve, fade, zoom_in, zoom_out, pan_left, pan_right)
            duration: ì „í™˜ ê¸¸ì´ (ì´ˆ)

        Returns:
            ì „í™˜ í´ë¦½
        """
        from moviepy import CompositeVideoClip, concatenate_videoclips

        try:
            if effect == "dissolve":
                # Crossfade íš¨ê³¼
                from_clip_end = from_clip.subclip(max(0, from_clip.duration - duration), from_clip.duration)
                to_clip_start = to_clip.subclip(0, min(duration, to_clip.duration))

                # Fade outê³¼ fade in í•©ì„±
                from_clip_fading = from_clip_end.fadein(0).fadeout(duration)
                to_clip_fading = to_clip_start.fadein(duration).fadeout(0)

                transition = CompositeVideoClip([
                    from_clip_fading,
                    to_clip_fading.set_start(0)
                ]).set_duration(duration)

                return transition

            elif effect == "fade":
                # ê²€ì€ í™”ë©´ì„ í†µí•œ í˜ì´ë“œ ì „í™˜
                from moviepy import ColorClip

                fade_duration = duration / 2
                from_clip_fade = from_clip.subclip(max(0, from_clip.duration - fade_duration), from_clip.duration).fadeout(fade_duration)
                to_clip_fade = to_clip.subclip(0, min(fade_duration, to_clip.duration)).fadein(fade_duration)

                # ë‘ í´ë¦½ì„ ì—°ê²°
                transition = concatenate_videoclips([from_clip_fade, to_clip_fade], method="compose")
                return transition.set_duration(duration)

            elif effect == "zoom_in":
                # ì¤Œì¸ íš¨ê³¼ (ì²« ë²ˆì§¸ í´ë¦½ì˜ ë§ˆì§€ë§‰ í”„ë ˆì„ì—ì„œ ì‹œì‘)
                # ê°„ë‹¨í•œ êµ¬í˜„: to_clipì„ ì„œì„œíˆ í¬ê²Œ ì‹œì‘
                to_clip_short = to_clip.subclip(0, min(duration, to_clip.duration))

                def zoom_effect(get_frame, t):
                    # t: 0 to duration
                    scale = 0.8 + (0.2 * (t / duration))  # 0.8ì—ì„œ 1.0ìœ¼ë¡œ í™•ëŒ€
                    frame = get_frame(t)
                    from PIL import Image
                    import numpy as np

                    img = Image.fromarray(frame)
                    w, h = img.size
                    new_w, new_h = int(w * scale), int(h * scale)

                    # ë¦¬ì‚¬ì´ì¦ˆ í›„ ì¤‘ì•™ í¬ë¡­
                    img_resized = img.resize((new_w, new_h), Image.Resampling.LANCZOS)

                    # ì¤‘ì•™ í¬ë¡­í•˜ì—¬ ì›ë˜ í¬ê¸°ë¡œ
                    left = (new_w - w) // 2
                    top = (new_h - h) // 2

                    if new_w < w or new_h < h:
                        # íŒ¨ë”© í•„ìš”
                        result = Image.new('RGB', (w, h), (0, 0, 0))
                        result.paste(img_resized, ((w - new_w) // 2, (h - new_h) // 2))
                        return np.array(result)
                    else:
                        img_cropped = img_resized.crop((left, top, left + w, top + h))
                        return np.array(img_cropped)

                # ê°„ë‹¨í•˜ê²Œ ê·¸ëƒ¥ í˜ì´ë“œì¸ìœ¼ë¡œ ëŒ€ì²´ (ë³µì¡í•œ zoom íš¨ê³¼ëŠ” êµ¬í˜„ ì–´ë ¤ì›€)
                return to_clip_short.fadein(duration * 0.3)

            elif effect == "zoom_out":
                # ì¤Œì•„ì›ƒ íš¨ê³¼
                to_clip_short = to_clip.subclip(0, min(duration, to_clip.duration))
                return to_clip_short.fadein(duration * 0.3)

            elif effect in ["pan_left", "pan_right"]:
                # íŒ¨ë‹ íš¨ê³¼ (ê°„ë‹¨í•œ êµ¬í˜„: í˜ì´ë“œ ì „í™˜)
                to_clip_short = to_clip.subclip(0, min(duration, to_clip.duration))
                return to_clip_short.fadein(duration * 0.3)

            else:
                # ê¸°ë³¸: ê°„ë‹¨í•œ í¬ë¡œìŠ¤í˜ì´ë“œ
                to_clip_short = to_clip.subclip(0, min(duration, to_clip.duration))
                return to_clip_short.fadein(min(0.5, duration))

        except Exception as e:
            logger.error(f"Error creating FFmpeg transition effect '{effect}': {str(e)}")
            # í´ë°±: ê°„ë‹¨í•œ í˜ì´ë“œì¸
            to_clip_short = to_clip.subclip(0, min(duration, to_clip.duration))
            return to_clip_short.fadein(0.5)

    async def compose_final_video(
        self,
        job: VideoGenerationJob,
        storyboard: List[Dict[str, Any]],
        images: List[Dict[str, str]],
        transition_videos: List[Dict[str, str]],
        db: Session
    ) -> str:
        """
        ìµœì¢… ë¹„ë””ì˜¤ í•©ì„± (AI ì „í™˜ + FFmpeg ì „í™˜ í˜¼í•©)

        Args:
            job: VideoGenerationJob ì¸ìŠ¤í„´ìŠ¤
            storyboard: ìŠ¤í† ë¦¬ë³´ë“œ ë°ì´í„° (ì»·ê³¼ ì „í™˜ì´ í˜¼í•©ëœ ë°°ì—´)
            images: ìƒì„±ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            transition_videos: ìƒì„±ëœ AI íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ (Kling)
            db: Database session

        Returns:
            str: ìµœì¢… ë¹„ë””ì˜¤ URL
        """
        import tempfile
        import os
        from moviepy import (
            ImageClip,
            VideoFileClip,
            concatenate_videoclips,
            CompositeVideoClip
        )
        from PIL import Image
        import io

        try:
            # Job ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = "composing"
            job.current_step = "Composing final video with mixed transitions"
            db.commit()

            logger.info(f"Starting video composition for job {job.id}")

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = tempfile.mkdtemp()
            logger.info(f"Created temp directory: {temp_dir}")

            # ìŠ¤í† ë¦¬ë³´ë“œì—ì„œ ì»·ê³¼ ì „í™˜ ë¶„ë¦¬
            cuts = [item for item in storyboard if 'cut' in item]
            transitions_data = {}  # {index: transition_info}

            for i, item in enumerate(storyboard):
                if 'transition' in item:
                    transitions_data[i] = item['transition']

            logger.info(f"Storyboard: {len(cuts)} cuts, {len(transitions_data)} transitions")

            # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§ ë° ë§¤í•‘
            image_by_cut = {img['cut']: img for img in images if img.get('url')}

            if not image_by_cut:
                raise ValueError("No valid images to compose video")

            # AI íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ë§¤í•‘ (Kling)
            ai_videos = {
                tv['transition']: tv
                for tv in transition_videos
                if tv.get('url')
            }

            logger.info(f"Processing {len(image_by_cut)} images, {len(ai_videos)} AI-generated transitions")

            clips = []
            image_clips_cache = {}  # ì´ë¯¸ì§€ í´ë¦½ ìºì‹œ (FFmpeg ì „í™˜ì— ì¬ì‚¬ìš©)

            # ìŠ¤í† ë¦¬ë³´ë“œ ìˆœì„œëŒ€ë¡œ í´ë¦½ ìƒì„±
            for i, item in enumerate(storyboard):
                if 'cut' in item:
                    # ì»· ì²˜ë¦¬
                    cut = item
                    cut_number = cut['cut']

                    if cut_number not in image_by_cut:
                        logger.warning(f"Image for cut {cut_number} not found, skipping")
                        continue

                    try:
                        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                        image_path = os.path.join(temp_dir, f"image_{cut_number}.jpg")
                        image_bytes = await self._download_file(image_by_cut[cut_number]['url'])

                        with open(image_path, 'wb') as f:
                            f.write(image_bytes)

                        # ImageClip ìƒì„±
                        # ì»· ì´ë¯¸ì§€ëŠ” í‚¤ í”„ë ˆì„ë§Œ ì§§ê²Œ í‘œì‹œ (0.3ì´ˆ ê³ ì •)
                        duration = 0.3
                        image_clip = ImageClip(image_path, duration=duration)
                        clips.append(image_clip)

                        # ìºì‹œì— ì €ì¥ (FFmpeg ì „í™˜ìš©)
                        image_clips_cache[cut_number] = image_clip

                        logger.info(f"Added image clip {cut_number} with duration {duration}s")

                    except Exception as e:
                        logger.error(f"Error processing image {cut_number}: {str(e)}")
                        continue

                elif 'transition' in item:
                    # ì „í™˜ ì²˜ë¦¬
                    transition = item['transition']
                    method = transition.get('method', 'ffmpeg')
                    effect = transition.get('effect', 'dissolve')
                    duration = transition.get('duration', 1.0)

                    # ì•ë’¤ ì»· ì°¾ê¸°
                    from_cut = None
                    to_cut = None

                    for j in range(i - 1, -1, -1):
                        if 'cut' in storyboard[j]:
                            from_cut = storyboard[j]['cut']
                            break

                    for j in range(i + 1, len(storyboard)):
                        if 'cut' in storyboard[j]:
                            to_cut = storyboard[j]['cut']
                            break

                    if not from_cut or not to_cut:
                        logger.warning(f"Could not determine from/to cuts for transition, skipping")
                        continue

                    transition_key = f"{from_cut}-{to_cut}"

                    try:
                        if method in ["veo", "minimax", "kling"]:
                            # AI ìƒì„± ë¹„ë””ì˜¤ ì‚¬ìš© (Kling)
                            if transition_key in ai_videos:
                                transition_path = os.path.join(temp_dir, f"ai_transition_{transition_key}.mp4")
                                video_bytes = await self._download_file(ai_videos[transition_key]['url'])

                                with open(transition_path, 'wb') as f:
                                    f.write(video_bytes)

                                transition_clip = VideoFileClip(transition_path)
                                clips.append(transition_clip)

                                logger.info(f"Added {method.upper()} transition {transition_key} ({effect})")
                            else:
                                logger.warning(f"{method.upper()} video for {transition_key} not found, falling back to FFmpeg")
                                # FFmpeg í´ë°±
                                if from_cut in image_clips_cache and to_cut in image_clips_cache:
                                    ffmpeg_transition = self._create_ffmpeg_transition(
                                        image_clips_cache[from_cut],
                                        image_clips_cache[to_cut],
                                        effect,
                                        duration
                                    )
                                    clips.append(ffmpeg_transition)
                                    logger.info(f"Added FFmpeg fallback transition {transition_key} ({effect})")

                        elif method == "ffmpeg":
                            # FFmpeg ì „í™˜ ìƒì„±
                            if from_cut in image_clips_cache and to_cut in image_clips_cache:
                                ffmpeg_transition = self._create_ffmpeg_transition(
                                    image_clips_cache[from_cut],
                                    image_clips_cache[to_cut],
                                    effect,
                                    duration
                                )
                                clips.append(ffmpeg_transition)
                                logger.info(f"Added FFmpeg transition {transition_key} ({effect})")
                            else:
                                logger.warning(f"Image clips for {transition_key} not found in cache")

                        else:
                            # ì˜ˆìƒì¹˜ ëª»í•œ method ê°’ - FFmpegë¡œ í´ë°±
                            logger.warning(f"Unknown transition method '{method}' for {transition_key}, falling back to FFmpeg")
                            if from_cut in image_clips_cache and to_cut in image_clips_cache:
                                ffmpeg_transition = self._create_ffmpeg_transition(
                                    image_clips_cache[from_cut],
                                    image_clips_cache[to_cut],
                                    effect,
                                    duration
                                )
                                clips.append(ffmpeg_transition)
                                logger.info(f"Added FFmpeg fallback transition {transition_key} (unknown method: {method})")

                    except Exception as e:
                        logger.error(f"Error processing transition {transition_key}: {str(e)}")
                        # ì „í™˜ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

            if not clips:
                raise ValueError("No clips to compose")

            # Job ìƒíƒœ ì—…ë°ì´íŠ¸
            job.current_step = f"Concatenating {len(clips)} clips"
            db.commit()

            # ëª¨ë“  í´ë¦½ í•©ì„±
            logger.info(f"Concatenating {len(clips)} clips...")
            final_video = concatenate_videoclips(clips, method="compose")

            # ìµœì¢… ë¹„ë””ì˜¤ ì €ì¥
            output_path = os.path.join(temp_dir, f"final_video_{job.id}.mp4")
            logger.info(f"Writing final video to {output_path}")

            job.current_step = "Rendering final video"
            db.commit()

            final_video.write_videofile(
                output_path,
                fps=30,
                codec='libx264',
                audio=False,
                preset='medium',
                threads=4
            )

            logger.info(f"Final video rendered: {output_path}")

            # Supabase Storageì— ì—…ë¡œë“œ
            job.current_step = "Uploading final video to Supabase Storage"
            db.commit()

            with open(output_path, 'rb') as f:
                video_url = await self._upload_final_to_supabase(
                    f.read(),
                    job.user_id,
                    job.id,
                    job.session_id
                )

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            logger.info("Cleaning up temporary files...")
            final_video.close()
            for clip in clips:
                try:
                    clip.close()
                except:
                    pass

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
            import shutil
            shutil.rmtree(temp_dir)
            logger.info("Temporary files cleaned up")

            # Job ì—…ë°ì´íŠ¸
            from sqlalchemy import func
            job.final_video_url = video_url
            job.status = "completed"
            job.current_step = "Video generation completed"
            job.completed_at = func.now()
            db.commit()

            # GeneratedVideo ë ˆì½”ë“œ ìƒì„± (ì™„ë£Œëœ ë¹„ë””ì˜¤ ê²°ê³¼ ì €ì¥)
            from app import models
            generated_video = models.GeneratedVideo(
                session_id=job.session_id,
                user_id=job.user_id,
                final_video_url=video_url,
                product_name=job.product_name,
                tier=job.tier,
                duration_seconds=job.duration_seconds
            )
            db.add(generated_video)
            db.commit()
            logger.info(f"GeneratedVideo record created for session {job.session_id}")

            logger.info(f"Video composition completed for job {job.id}: {video_url}")
            return video_url

        except Exception as e:
            logger.error(f"Error in video composition for job {job.id}: {str(e)}")
            job.status = "failed"
            job.error_message = f"Video composition failed: {str(e)}"
            db.commit()

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹œë„
            try:
                if 'temp_dir' in locals():
                    import shutil
                    shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass

            raise

    async def _download_file(self, url: str) -> bytes:
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” HTTP)
        - ìƒëŒ€ ê²½ë¡œ(/uploads/...)ì¸ ê²½ìš°: ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì§ì ‘ ì½ê¸°
        - ì ˆëŒ€ URL(http://, https://)ì¸ ê²½ìš°: HTTPë¡œ ë‹¤ìš´ë¡œë“œ
        """
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì§ì ‘ ì½ê¸°
        if url.startswith('/uploads/'):
            try:
                # /uploads/ ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œ ê²½ë¡œë¡œ ë³€í™˜
                # íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© (í™˜ê²½ ë…ë¦½ì )
                # backend/app/services/ â†’ backend/app/ â†’ backend/ â†’ í”„ë¡œì íŠ¸ë£¨íŠ¸/
                file_path = Path(__file__).parent.parent.parent / url.lstrip('/')

                logger.info(f"Reading file from local filesystem: {file_path}")

                with open(file_path, 'rb') as f:
                    file_data = f.read()

                logger.info(f"Successfully read local file: {len(file_data)} bytes")
                return file_data

            except Exception as e:
                logger.error(f"Failed to read local file {file_path}: {str(e)}")
                raise
        else:
            # ì ˆëŒ€ URLì¸ ê²½ìš° HTTPë¡œ ë‹¤ìš´ë¡œë“œ
            try:
                logger.info(f"Downloading file from URL: {url}")
                async with httpx.AsyncClient() as client:
                    response = await client.get(url)
                    response.raise_for_status()
                    logger.info(f"Successfully downloaded file: {len(response.content)} bytes")
                    return response.content
            except Exception as e:
                logger.error(f"Failed to download file from {url}: {str(e)}")
                raise

    async def _upload_final_to_supabase(
        self,
        video_data: bytes,
        user_id: int,
        job_id: int,
        session_id: str
    ) -> str:
        """ìµœì¢… ë¹„ë””ì˜¤ë¥¼ Supabase Storageì— ì €ì¥"""
        try:
            from app.services.supabase_storage import get_storage_service
            storage = get_storage_service()

            # íŒŒì¼ ê²½ë¡œ ìƒì„±
            file_path = f"{user_id}/{session_id}.mp4"

            # Supabase Storageì— ì—…ë¡œë“œ
            file_url = storage.upload_file(
                bucket="ai-video-finals",
                file_path=file_path,
                file_data=video_data,
                content_type="video/mp4"
            )

            logger.info(f"Final video saved to Supabase Storage: {file_url}")
            return file_url
        except Exception as e:
            logger.error(f"Failed to save final video to Supabase Storage: {str(e)}")
            raise


# ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def run_video_generation_pipeline(job_id: int, db: Session):
    """
    ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    1. Master Planning Agent: ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±
    2. Image Generation: ê° ì»·ì˜ ì´ë¯¸ì§€ ìƒì„±
    3. Video Generation: ì»· ì‚¬ì´ íŠ¸ëœì§€ì…˜ ë¹„ë””ì˜¤ ìƒì„±
    4. Video Composition: ìµœì¢… ë¹„ë””ì˜¤ í•©ì„±
    """
    try:
        # Job ì¡°íšŒ
        job = db.query(VideoGenerationJob).filter(VideoGenerationJob.id == job_id).first()
        if not job:
            logger.error(f"Job {job_id} not found")
            return

        # User ì¡°íšŒ
        user = db.query(User).filter(User.id == job.user_id).first()
        if not user:
            logger.error(f"User {job.user_id} not found for job {job_id}")
            job.status = "failed"
            job.error_message = "User not found"
            db.commit()
            return

        # BrandAnalysis ì¡°íšŒ (ìˆëŠ” ê²½ìš°)
        brand_analysis = db.query(BrandAnalysis).filter(
            BrandAnalysis.user_id == user.id
        ).first()

        # 1. Master Planning Agent ì‹¤í–‰
        logger.info(f"Step 1/4: Running Master Planning Agent for job {job_id}")
        planning_agent = MasterPlanningAgent()
        storyboard_result = await planning_agent.analyze_and_plan(job, user, brand_analysis, db)

        # ìƒˆ êµ¬ì¡°: {shared_visual_context: {...}, storyboard: [...]}
        storyboard = storyboard_result.get("storyboard", storyboard_result if isinstance(storyboard_result, list) else [])

        # 2. Image Generation
        cuts = [item for item in storyboard if 'cut' in item]
        logger.info(f"Step 2/4: Generating images for {len(cuts)} cuts")
        image_agent = ImageGenerationAgent()
        images = await image_agent.generate_images(job, storyboard, db)

        # 3. Video Generation (Kling 2.1 Standard transitions via fal.ai)
        logger.info(f"Step 3/4: Generating Kling 2.1 transition videos")
        video_agent = KlingVideoGenerationAgent()  # â† Kling 2.1 Standard via fal.ai
        videos = await video_agent.generate_transition_videos(job, storyboard, images, db)

        # 4. Video Composition (mixed: Kling + FFmpeg transitions)
        logger.info(f"Step 4/4: Composing final video with mixed transitions")
        composition_agent = VideoCompositionAgent()
        final_video_url = await composition_agent.compose_final_video(
            job, storyboard, images, videos, db
        )

        logger.info(f"Video generation pipeline completed for job {job_id}: {final_video_url}")

    except Exception as e:
        logger.error(f"Error in video generation pipeline for job {job_id}: {str(e)}")
        if job:
            job.status = "failed"
            job.error_message = str(e)
            db.commit()
